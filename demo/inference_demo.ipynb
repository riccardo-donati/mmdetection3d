{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from mmdet3d.apis import LidarDet3DInferencer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot find model: pointpillars_kittilidar-car in mmdet3d",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# initialize inferencer\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# CHANGE THE center_mode in loca_visualizer accordingly (KITTI -> center_mode = 'lidar_bottom', CUSTOM -> center_mode = whatever)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#inferencer = LidarDet3DInferencer('pointpillars_donaset-car')\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m inferencer \u001b[38;5;241m=\u001b[39m \u001b[43mLidarDet3DInferencer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpointpillars_kittilidar-car\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# inferencer = LidarDet3DInferencer(\"pointpillars_hv_secfpn_8xb6-160e_kittilidar-3d-car\",scope=\"mmdet3d\")\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mmdet3d/apis/inferencers/lidar_det3d_inferencer.py:60\u001b[0m, in \u001b[0;36mLidarDet3DInferencer.__init__\u001b[0;34m(self, model, weights, device, scope, palette)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     52\u001b[0m              model: Union[ModelType, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     53\u001b[0m              weights: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# A global counter tracking the number of frames processed, for\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# naming of the output results\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_visualized_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mLidarDet3DInferencer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpalette\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpalette\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mmdet3d/apis/inferencers/base_3d_inferencer.py:72\u001b[0m, in \u001b[0;36mBase3DInferencer.__init__\u001b[0;34m(self, model, weights, device, scope, palette)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpalette \u001b[38;5;241m=\u001b[39m palette\n\u001b[1;32m     71\u001b[0m init_default_scope(scope)\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m revert_sync_batchnorm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mmengine/infer/infer.py:160\u001b[0m, in \u001b[0;36mBaseInferencer.__init__\u001b[0;34m(self, model, weights, device, scope, show_progress)\u001b[0m\n\u001b[1;32m    156\u001b[0m     cfg \u001b[38;5;241m=\u001b[39m Config\u001b[38;5;241m.\u001b[39mfromfile(model)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# Load config and weights from metafile. If `weights` is\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# assigned, the weights defined in metafile will be ignored.\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     cfg, _weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model_from_metafile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m         weights \u001b[38;5;241m=\u001b[39m _weights\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/mmengine/infer/infer.py:393\u001b[0m, in \u001b[0;36mBaseInferencer._load_model_from_metafile\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    391\u001b[0m         weights \u001b[38;5;241m=\u001b[39m weights[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weights, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m weights\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cfg, weights\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot find model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscope\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot find model: pointpillars_kittilidar-car in mmdet3d"
     ]
    }
   ],
   "source": [
    "# initialize inferencer\n",
    "# CHANGE THE center_mode in loca_visualizer accordingly (KITTI -> center_mode = 'lidar_bottom', CUSTOM -> center_mode = whatever)\n",
    "\n",
    "#inferencer = LidarDet3DInferencer('pointpillars_donaset-car')\n",
    "inferencer = LidarDet3DInferencer('pointpillars_kittilidar-car') \n",
    "# inferencer = LidarDet3DInferencer(\"pointpillars_hv_secfpn_8xb6-160e_kittilidar-3d-car\",scope=\"mmdet3d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['self',\n",
       " 'inputs',\n",
       " 'preds',\n",
       " 'return_vis',\n",
       " 'show',\n",
       " 'wait_time',\n",
       " 'draw_pred',\n",
       " 'pred_score_thr',\n",
       " 'no_save_vis',\n",
       " 'img_out_dir']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "args = inspect.getfullargspec(inferencer.visualize).args\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# inference\n",
    "pcl = '../data/KITTI_lidar/testing/velodyne/'\n",
    "# pcl = './data/falcon/falcon1.bin'\n",
    "\n",
    "inputs = dict(points=pcl)\n",
    "# inferencer(inputs, show=False,pred_score_thr=0.3, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing took 97.84793853759766 ms\n",
      "Voxel Encoder took 51.92255973815918 ms\n",
      "Middle Encoder/Imagify took 15.018463134765625 ms\n",
      "Backbone + FPN took 200.74939727783203 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riccardo/venvs/pytorch_cuda123_env/lib/python3.10/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bbox Head pred took 376.6789436340332 ms\n",
      "\n",
      "Preprocessing took 56.319236755371094 ms\n",
      "Voxel Encoder took 0.6341934204101562 ms\n",
      "Middle Encoder/Imagify took 23.07915687561035 ms\n",
      "Backbone + FPN took 1.0592937469482422 ms\n",
      "Bbox Head pred took 65.92702865600586 ms\n",
      "\n",
      "Preprocessing took 48.96140098571777 ms\n",
      "Voxel Encoder took 1.6794204711914062 ms\n",
      "Middle Encoder/Imagify took 31.2654972076416 ms\n",
      "Backbone + FPN took 0.9922981262207031 ms\n",
      "Bbox Head pred took 64.77046012878418 ms\n",
      "\n",
      "Preprocessing took 57.01875686645508 ms\n",
      "Voxel Encoder took 0.8955001831054688 ms\n",
      "Middle Encoder/Imagify took 33.78033638000488 ms\n",
      "Backbone + FPN took 1.1708736419677734 ms\n",
      "Bbox Head pred took 65.14739990234375 ms\n",
      "\n",
      "Preprocessing took 55.161476135253906 ms\n",
      "Voxel Encoder took 1.45721435546875 ms\n",
      "Middle Encoder/Imagify took 33.80250930786133 ms\n",
      "Backbone + FPN took 1.1229515075683594 ms\n",
      "Bbox Head pred took 65.35673141479492 ms\n",
      "\n",
      "Preprocessing took 51.36585235595703 ms\n",
      "Voxel Encoder took 0.9324550628662109 ms\n",
      "Middle Encoder/Imagify took 37.72377967834473 ms\n",
      "Backbone + FPN took 2.0601749420166016 ms\n",
      "Bbox Head pred took 66.21408462524414 ms\n",
      "\n",
      "Preprocessing took 56.824684143066406 ms\n",
      "Voxel Encoder took 0.7297992706298828 ms\n",
      "Middle Encoder/Imagify took 19.154071807861328 ms\n",
      "Backbone + FPN took 1.0192394256591797 ms\n",
      "Bbox Head pred took 65.03510475158691 ms\n",
      "\n",
      "Preprocessing took 57.92951583862305 ms\n",
      "Voxel Encoder took 0.6287097930908203 ms\n",
      "Middle Encoder/Imagify took 37.66822814941406 ms\n",
      "Backbone + FPN took 2.752065658569336 ms\n",
      "Bbox Head pred took 64.09382820129395 ms\n",
      "\n",
      "Preprocessing took 56.696414947509766 ms\n",
      "Voxel Encoder took 0.5028247833251953 ms\n",
      "Middle Encoder/Imagify took 35.5682373046875 ms\n",
      "Backbone + FPN took 0.9784698486328125 ms\n",
      "Bbox Head pred took 66.63346290588379 ms\n",
      "\n",
      "Preprocessing took 52.34789848327637 ms\n",
      "Voxel Encoder took 0.5383491516113281 ms\n",
      "Middle Encoder/Imagify took 37.828922271728516 ms\n",
      "Backbone + FPN took 1.1262893676757812 ms\n",
      "Bbox Head pred took 65.40584564208984 ms\n",
      "\n",
      "Preprocessing took 48.59423637390137 ms\n",
      "Voxel Encoder took 0.7703304290771484 ms\n",
      "Middle Encoder/Imagify took 20.71523666381836 ms\n",
      "Backbone + FPN took 0.9868144989013672 ms\n",
      "Bbox Head pred took 64.93711471557617 ms\n",
      "\n",
      "Preprocessing took 56.75959587097168 ms\n",
      "Voxel Encoder took 0.5321502685546875 ms\n",
      "Middle Encoder/Imagify took 31.383991241455078 ms\n",
      "Backbone + FPN took 0.9999275207519531 ms\n",
      "Bbox Head pred took 65.52457809448242 ms\n",
      "\n",
      "Preprocessing took 57.00421333312988 ms\n",
      "Voxel Encoder took 0.5207061767578125 ms\n",
      "Middle Encoder/Imagify took 24.195194244384766 ms\n",
      "Backbone + FPN took 1.0166168212890625 ms\n",
      "Bbox Head pred took 64.78619575500488 ms\n",
      "\n",
      "Preprocessing took 58.803558349609375 ms\n",
      "Voxel Encoder took 0.9334087371826172 ms\n",
      "Middle Encoder/Imagify took 31.203746795654297 ms\n",
      "Backbone + FPN took 0.9872913360595703 ms\n",
      "Bbox Head pred took 65.57106971740723 ms\n",
      "\n",
      "Preprocessing took 52.437782287597656 ms\n",
      "Voxel Encoder took 0.5159378051757812 ms\n",
      "Middle Encoder/Imagify took 30.18021583557129 ms\n",
      "Backbone + FPN took 1.9159317016601562 ms\n",
      "Bbox Head pred took 64.24546241760254 ms\n",
      "\n",
      "Preprocessing took 53.679704666137695 ms\n",
      "Voxel Encoder took 0.5297660827636719 ms\n",
      "Middle Encoder/Imagify took 32.94062614440918 ms\n",
      "Backbone + FPN took 1.0378360748291016 ms\n",
      "Bbox Head pred took 65.50216674804688 ms\n",
      "\n",
      "Preprocessing took 55.40633201599121 ms\n",
      "Voxel Encoder took 0.5841255187988281 ms\n",
      "Middle Encoder/Imagify took 20.93219757080078 ms\n",
      "Backbone + FPN took 0.9968280792236328 ms\n",
      "Bbox Head pred took 64.68653678894043 ms\n",
      "\n",
      "Preprocessing took 50.86374282836914 ms\n",
      "Voxel Encoder took 0.6527900695800781 ms\n",
      "Middle Encoder/Imagify took 27.894973754882812 ms\n",
      "Backbone + FPN took 1.0223388671875 ms\n",
      "Bbox Head pred took 64.23115730285645 ms\n",
      "\n",
      "Preprocessing took 56.1680793762207 ms\n",
      "Voxel Encoder took 0.5159378051757812 ms\n",
      "Middle Encoder/Imagify took 37.30964660644531 ms\n",
      "Backbone + FPN took 1.2359619140625 ms\n",
      "Bbox Head pred took 65.51909446716309 ms\n",
      "\n",
      "Preprocessing took 56.925296783447266 ms\n",
      "Voxel Encoder took 0.5424022674560547 ms\n",
      "Middle Encoder/Imagify took 32.69386291503906 ms\n",
      "Backbone + FPN took 1.085042953491211 ms\n",
      "Bbox Head pred took 66.04552268981934 ms\n",
      "\n",
      "Preprocessing took 51.42855644226074 ms\n",
      "Voxel Encoder took 0.5147457122802734 ms\n",
      "Middle Encoder/Imagify took 26.16095542907715 ms\n",
      "Backbone + FPN took 0.9796619415283203 ms\n",
      "Bbox Head pred took 64.36586380004883 ms\n",
      "\n",
      "Preprocessing took 56.989431381225586 ms\n",
      "Voxel Encoder took 0.7877349853515625 ms\n",
      "Middle Encoder/Imagify took 31.84199333190918 ms\n",
      "Backbone + FPN took 1.1014938354492188 ms\n",
      "Bbox Head pred took 64.96715545654297 ms\n",
      "\n",
      "Preprocessing took 55.28378486633301 ms\n",
      "Voxel Encoder took 1.1794567108154297 ms\n",
      "Middle Encoder/Imagify took 40.86875915527344 ms\n",
      "Backbone + FPN took 1.1854171752929688 ms\n",
      "Bbox Head pred took 64.67533111572266 ms\n",
      "\n",
      "Preprocessing took 51.89990997314453 ms\n",
      "Voxel Encoder took 0.5035400390625 ms\n",
      "Middle Encoder/Imagify took 25.896072387695312 ms\n",
      "Backbone + FPN took 1.070261001586914 ms\n",
      "Bbox Head pred took 64.4831657409668 ms\n",
      "\n",
      "Preprocessing took 58.5477352142334 ms\n",
      "Voxel Encoder took 0.5207061767578125 ms\n",
      "Middle Encoder/Imagify took 30.61223030090332 ms\n",
      "Backbone + FPN took 0.9908676147460938 ms\n",
      "Bbox Head pred took 65.40250778198242 ms\n",
      "\n",
      "Preprocessing took 49.9415397644043 ms\n",
      "Voxel Encoder took 0.7753372192382812 ms\n",
      "Middle Encoder/Imagify took 22.60279655456543 ms\n",
      "Backbone + FPN took 0.9758472442626953 ms\n",
      "Bbox Head pred took 65.60134887695312 ms\n",
      "\n",
      "Preprocessing took 50.312042236328125 ms\n",
      "Voxel Encoder took 0.5130767822265625 ms\n",
      "Middle Encoder/Imagify took 17.474651336669922 ms\n",
      "Backbone + FPN took 0.9973049163818359 ms\n",
      "Bbox Head pred took 65.87386131286621 ms\n",
      "\n",
      "Preprocessing took 50.97055435180664 ms\n",
      "Voxel Encoder took 0.7483959197998047 ms\n",
      "Middle Encoder/Imagify took 29.857635498046875 ms\n",
      "Backbone + FPN took 1.0645389556884766 ms\n",
      "Bbox Head pred took 65.52386283874512 ms\n",
      "\n",
      "Preprocessing took 54.062843322753906 ms\n",
      "Voxel Encoder took 0.6568431854248047 ms\n",
      "Middle Encoder/Imagify took 25.882482528686523 ms\n",
      "Backbone + FPN took 1.0302066802978516 ms\n",
      "Bbox Head pred took 65.66882133483887 ms\n",
      "\n",
      "Preprocessing took 52.9787540435791 ms\n",
      "Voxel Encoder took 0.5238056182861328 ms\n",
      "Middle Encoder/Imagify took 37.172794342041016 ms\n",
      "Backbone + FPN took 1.066446304321289 ms\n",
      "Bbox Head pred took 66.12396240234375 ms\n",
      "\n",
      "Preprocessing took 54.551124572753906 ms\n",
      "Voxel Encoder took 0.5078315734863281 ms\n",
      "Middle Encoder/Imagify took 25.72178840637207 ms\n",
      "Backbone + FPN took 0.9829998016357422 ms\n",
      "Bbox Head pred took 64.79811668395996 ms\n",
      "\n",
      "Preprocessing took 55.17697334289551 ms\n",
      "Voxel Encoder took 0.5383491516113281 ms\n",
      "Middle Encoder/Imagify took 31.293392181396484 ms\n",
      "Backbone + FPN took 1.0404586791992188 ms\n",
      "Bbox Head pred took 64.74161148071289 ms\n",
      "\n",
      "Preprocessing took 54.63218688964844 ms\n",
      "Voxel Encoder took 0.7944107055664062 ms\n",
      "Middle Encoder/Imagify took 20.060062408447266 ms\n",
      "Backbone + FPN took 1.0237693786621094 ms\n",
      "Bbox Head pred took 65.37795066833496 ms\n",
      "\n",
      "Preprocessing took 57.23428726196289 ms\n",
      "Voxel Encoder took 0.5359649658203125 ms\n",
      "Middle Encoder/Imagify took 29.484272003173828 ms\n",
      "Backbone + FPN took 0.9560585021972656 ms\n",
      "Bbox Head pred took 66.02716445922852 ms\n",
      "\n",
      "Preprocessing took 52.796125411987305 ms\n",
      "Voxel Encoder took 0.5204677581787109 ms\n",
      "Middle Encoder/Imagify took 20.623207092285156 ms\n",
      "Backbone + FPN took 0.9737014770507812 ms\n",
      "Bbox Head pred took 65.40274620056152 ms\n",
      "\n",
      "Preprocessing took 55.730342864990234 ms\n",
      "Voxel Encoder took 0.7424354553222656 ms\n",
      "Middle Encoder/Imagify took 18.163442611694336 ms\n",
      "Backbone + FPN took 1.0137557983398438 ms\n",
      "Bbox Head pred took 64.8488998413086 ms\n",
      "\n",
      "Preprocessing took 58.03537368774414 ms\n",
      "Voxel Encoder took 0.5283355712890625 ms\n",
      "Middle Encoder/Imagify took 29.137611389160156 ms\n",
      "Backbone + FPN took 1.033782958984375 ms\n",
      "Bbox Head pred took 65.49215316772461 ms\n",
      "\n",
      "Preprocessing took 56.78391456604004 ms\n",
      "Voxel Encoder took 0.5431175231933594 ms\n",
      "Middle Encoder/Imagify took 23.94700050354004 ms\n",
      "Backbone + FPN took 0.9787082672119141 ms\n",
      "Bbox Head pred took 65.72604179382324 ms\n",
      "\n",
      "Preprocessing took 56.764841079711914 ms\n",
      "Voxel Encoder took 0.8809566497802734 ms\n",
      "Middle Encoder/Imagify took 27.519702911376953 ms\n",
      "Backbone + FPN took 1.0783672332763672 ms\n",
      "Bbox Head pred took 66.29467010498047 ms\n",
      "\n",
      "Preprocessing took 52.63996124267578 ms\n",
      "Voxel Encoder took 0.5671977996826172 ms\n",
      "Middle Encoder/Imagify took 35.52722930908203 ms\n",
      "Backbone + FPN took 1.3358592987060547 ms\n",
      "Bbox Head pred took 66.2696361541748 ms\n",
      "\n",
      "Preprocessing took 52.75774002075195 ms\n",
      "Voxel Encoder took 0.5509853363037109 ms\n",
      "Middle Encoder/Imagify took 28.242826461791992 ms\n",
      "Backbone + FPN took 1.0156631469726562 ms\n",
      "Bbox Head pred took 69.35620307922363 ms\n",
      "\n",
      "Preprocessing took 53.58123779296875 ms\n",
      "Voxel Encoder took 0.6561279296875 ms\n",
      "Middle Encoder/Imagify took 23.255109786987305 ms\n",
      "Backbone + FPN took 1.2385845184326172 ms\n",
      "Bbox Head pred took 65.51051139831543 ms\n",
      "\n",
      "Preprocessing took 60.05454063415527 ms\n",
      "Voxel Encoder took 0.9076595306396484 ms\n",
      "Middle Encoder/Imagify took 40.12131690979004 ms\n",
      "Backbone + FPN took 1.009225845336914 ms\n",
      "Bbox Head pred took 66.03384017944336 ms\n",
      "\n",
      "Preprocessing took 54.39281463623047 ms\n",
      "Voxel Encoder took 0.5452632904052734 ms\n",
      "Middle Encoder/Imagify took 27.726411819458008 ms\n",
      "Backbone + FPN took 1.2736320495605469 ms\n",
      "Bbox Head pred took 67.59428977966309 ms\n",
      "\n",
      "Preprocessing took 51.68747901916504 ms\n",
      "Voxel Encoder took 0.5283355712890625 ms\n",
      "Middle Encoder/Imagify took 39.52145576477051 ms\n",
      "Backbone + FPN took 1.2624263763427734 ms\n",
      "Bbox Head pred took 66.50352478027344 ms\n",
      "\n",
      "Preprocessing took 56.905508041381836 ms\n",
      "Voxel Encoder took 0.6456375122070312 ms\n",
      "Middle Encoder/Imagify took 34.859418869018555 ms\n",
      "Backbone + FPN took 1.0819435119628906 ms\n",
      "Bbox Head pred took 66.49661064147949 ms\n",
      "\n",
      "Preprocessing took 56.57052993774414 ms\n",
      "Voxel Encoder took 0.7405281066894531 ms\n",
      "Middle Encoder/Imagify took 35.28714179992676 ms\n",
      "Backbone + FPN took 1.0399818420410156 ms\n",
      "Bbox Head pred took 66.8187141418457 ms\n",
      "\n",
      "Preprocessing took 58.03203582763672 ms\n",
      "Voxel Encoder took 0.5390644073486328 ms\n",
      "Middle Encoder/Imagify took 24.52850341796875 ms\n",
      "Backbone + FPN took 1.1630058288574219 ms\n",
      "Bbox Head pred took 65.58990478515625 ms\n",
      "\n",
      "Preprocessing took 56.86783790588379 ms\n",
      "Voxel Encoder took 0.5602836608886719 ms\n",
      "Middle Encoder/Imagify took 32.21702575683594 ms\n",
      "Backbone + FPN took 1.0752677917480469 ms\n",
      "Bbox Head pred took 68.11118125915527 ms\n",
      "\n",
      "Preprocessing took 49.82733726501465 ms\n",
      "Voxel Encoder took 0.5362033843994141 ms\n",
      "Middle Encoder/Imagify took 31.567811965942383 ms\n",
      "Backbone + FPN took 1.0509490966796875 ms\n",
      "Bbox Head pred took 66.42365455627441 ms\n",
      "\n",
      "Preprocessing took 57.37495422363281 ms\n",
      "Voxel Encoder took 0.6792545318603516 ms\n",
      "Middle Encoder/Imagify took 30.782461166381836 ms\n",
      "Backbone + FPN took 1.4195442199707031 ms\n",
      "Bbox Head pred took 65.81306457519531 ms\n",
      "\n",
      "Preprocessing took 58.65812301635742 ms\n",
      "Voxel Encoder took 0.6322860717773438 ms\n",
      "Middle Encoder/Imagify took 34.54709053039551 ms\n",
      "Backbone + FPN took 1.135110855102539 ms\n",
      "Bbox Head pred took 66.88046455383301 ms\n",
      "\n",
      "Preprocessing took 55.61661720275879 ms\n",
      "Voxel Encoder took 0.5385875701904297 ms\n",
      "Middle Encoder/Imagify took 28.711318969726562 ms\n",
      "Backbone + FPN took 0.9989738464355469 ms\n",
      "Bbox Head pred took 65.81807136535645 ms\n",
      "\n",
      "Preprocessing took 50.65608024597168 ms\n",
      "Voxel Encoder took 0.5412101745605469 ms\n",
      "Middle Encoder/Imagify took 21.301984786987305 ms\n",
      "Backbone + FPN took 1.0869503021240234 ms\n",
      "Bbox Head pred took 67.5048828125 ms\n",
      "\n",
      "Preprocessing took 51.97787284851074 ms\n",
      "Voxel Encoder took 0.6732940673828125 ms\n",
      "Middle Encoder/Imagify took 23.952007293701172 ms\n",
      "Backbone + FPN took 1.0161399841308594 ms\n",
      "Bbox Head pred took 65.4456615447998 ms\n",
      "\n",
      "Preprocessing took 56.94317817687988 ms\n",
      "Voxel Encoder took 0.6020069122314453 ms\n",
      "Middle Encoder/Imagify took 16.944408416748047 ms\n",
      "Backbone + FPN took 1.1339187622070312 ms\n",
      "Bbox Head pred took 65.155029296875 ms\n",
      "\n",
      "Preprocessing took 49.54934120178223 ms\n",
      "Voxel Encoder took 0.5590915679931641 ms\n",
      "Middle Encoder/Imagify took 29.381275177001953 ms\n",
      "Backbone + FPN took 1.0647773742675781 ms\n",
      "Bbox Head pred took 65.90080261230469 ms\n",
      "\n",
      "Preprocessing took 54.16703224182129 ms\n",
      "Voxel Encoder took 0.8087158203125 ms\n",
      "Middle Encoder/Imagify took 32.69839286804199 ms\n",
      "Backbone + FPN took 1.1761188507080078 ms\n",
      "Bbox Head pred took 65.30046463012695 ms\n",
      "\n",
      "Preprocessing took 52.616119384765625 ms\n",
      "Voxel Encoder took 0.5528926849365234 ms\n",
      "Middle Encoder/Imagify took 37.80698776245117 ms\n",
      "Backbone + FPN took 1.3244152069091797 ms\n",
      "Bbox Head pred took 65.30213356018066 ms\n",
      "\n",
      "Preprocessing took 57.294368743896484 ms\n",
      "Voxel Encoder took 0.5700588226318359 ms\n",
      "Middle Encoder/Imagify took 41.07522964477539 ms\n",
      "Backbone + FPN took 1.02996826171875 ms\n",
      "Bbox Head pred took 65.64664840698242 ms\n",
      "\n",
      "Preprocessing took 55.73916435241699 ms\n",
      "Voxel Encoder took 0.6887912750244141 ms\n",
      "Middle Encoder/Imagify took 21.998882293701172 ms\n",
      "Backbone + FPN took 1.1150836944580078 ms\n",
      "Bbox Head pred took 66.73121452331543 ms\n",
      "\n",
      "Preprocessing took 58.81333351135254 ms\n",
      "Voxel Encoder took 1.3616085052490234 ms\n",
      "Middle Encoder/Imagify took 34.91473197937012 ms\n",
      "Backbone + FPN took 1.0223388671875 ms\n",
      "Bbox Head pred took 66.45607948303223 ms\n",
      "\n",
      "Preprocessing took 50.284624099731445 ms\n",
      "Voxel Encoder took 0.5276203155517578 ms\n",
      "Middle Encoder/Imagify took 32.18507766723633 ms\n",
      "Backbone + FPN took 1.2624263763427734 ms\n",
      "Bbox Head pred took 66.5590763092041 ms\n",
      "\n",
      "Preprocessing took 54.471731185913086 ms\n",
      "Voxel Encoder took 0.5443096160888672 ms\n",
      "Middle Encoder/Imagify took 35.55130958557129 ms\n",
      "Backbone + FPN took 1.0442733764648438 ms\n",
      "Bbox Head pred took 66.5748119354248 ms\n",
      "\n",
      "Preprocessing took 55.64546585083008 ms\n",
      "Voxel Encoder took 0.5543231964111328 ms\n",
      "Middle Encoder/Imagify took 21.70872688293457 ms\n",
      "Backbone + FPN took 0.9915828704833984 ms\n",
      "Bbox Head pred took 65.78922271728516 ms\n",
      "\n",
      "Preprocessing took 57.76715278625488 ms\n",
      "Voxel Encoder took 0.7269382476806641 ms\n",
      "Middle Encoder/Imagify took 22.82094955444336 ms\n",
      "Backbone + FPN took 0.9896755218505859 ms\n",
      "Bbox Head pred took 66.04814529418945 ms\n",
      "\n",
      "Preprocessing took 49.41582679748535 ms\n",
      "Voxel Encoder took 0.8137226104736328 ms\n",
      "Middle Encoder/Imagify took 19.548416137695312 ms\n",
      "Backbone + FPN took 0.9708404541015625 ms\n",
      "Bbox Head pred took 67.45076179504395 ms\n",
      "\n",
      "Preprocessing took 53.78127098083496 ms\n",
      "Voxel Encoder took 0.5292892456054688 ms\n",
      "Middle Encoder/Imagify took 30.28106689453125 ms\n",
      "Backbone + FPN took 1.0030269622802734 ms\n",
      "Bbox Head pred took 64.32890892028809 ms\n",
      "\n",
      "Preprocessing took 53.46536636352539 ms\n",
      "Voxel Encoder took 0.8082389831542969 ms\n",
      "Middle Encoder/Imagify took 27.18043327331543 ms\n",
      "Backbone + FPN took 1.0173320770263672 ms\n",
      "Bbox Head pred took 66.70999526977539 ms\n",
      "\n",
      "Preprocessing took 54.985761642456055 ms\n",
      "Voxel Encoder took 0.5233287811279297 ms\n",
      "Middle Encoder/Imagify took 27.97412872314453 ms\n",
      "Backbone + FPN took 1.1816024780273438 ms\n",
      "Bbox Head pred took 65.86575508117676 ms\n",
      "\n",
      "Preprocessing took 57.511329650878906 ms\n",
      "Voxel Encoder took 0.6711483001708984 ms\n",
      "Middle Encoder/Imagify took 27.9543399810791 ms\n",
      "Backbone + FPN took 0.9794235229492188 ms\n",
      "Bbox Head pred took 66.55573844909668 ms\n",
      "\n",
      "Preprocessing took 52.82282829284668 ms\n",
      "Voxel Encoder took 0.5605220794677734 ms\n",
      "Middle Encoder/Imagify took 32.94086456298828 ms\n",
      "Backbone + FPN took 0.9982585906982422 ms\n",
      "Bbox Head pred took 67.07572937011719 ms\n",
      "\n",
      "Preprocessing took 55.9232234954834 ms\n",
      "Voxel Encoder took 0.8571147918701172 ms\n",
      "Middle Encoder/Imagify took 28.531789779663086 ms\n",
      "Backbone + FPN took 1.0716915130615234 ms\n",
      "Bbox Head pred took 67.29602813720703 ms\n",
      "\n",
      "Preprocessing took 45.76921463012695 ms\n",
      "Voxel Encoder took 0.5204677581787109 ms\n",
      "Middle Encoder/Imagify took 23.17667007446289 ms\n",
      "Backbone + FPN took 1.0025501251220703 ms\n",
      "Bbox Head pred took 64.41783905029297 ms\n",
      "\n",
      "Preprocessing took 56.067466735839844 ms\n",
      "Voxel Encoder took 0.5350112915039062 ms\n",
      "Middle Encoder/Imagify took 27.522802352905273 ms\n",
      "Backbone + FPN took 1.0039806365966797 ms\n",
      "Bbox Head pred took 66.63632392883301 ms\n",
      "\n",
      "Preprocessing took 59.145450592041016 ms\n",
      "Voxel Encoder took 2.386331558227539 ms\n",
      "Middle Encoder/Imagify took 25.483369827270508 ms\n",
      "Backbone + FPN took 1.031637191772461 ms\n",
      "Bbox Head pred took 66.56217575073242 ms\n",
      "\n",
      "Preprocessing took 52.83188819885254 ms\n",
      "Voxel Encoder took 0.5407333374023438 ms\n",
      "Middle Encoder/Imagify took 35.28165817260742 ms\n",
      "Backbone + FPN took 1.2993812561035156 ms\n",
      "Bbox Head pred took 67.81220436096191 ms\n",
      "\n",
      "Preprocessing took 58.43615531921387 ms\n",
      "Voxel Encoder took 0.5280971527099609 ms\n",
      "Middle Encoder/Imagify took 25.592565536499023 ms\n",
      "Backbone + FPN took 1.1181831359863281 ms\n",
      "Bbox Head pred took 66.28251075744629 ms\n",
      "\n",
      "Preprocessing took 56.45298957824707 ms\n",
      "Voxel Encoder took 0.518798828125 ms\n",
      "Middle Encoder/Imagify took 20.60556411743164 ms\n",
      "Backbone + FPN took 1.0752677917480469 ms\n",
      "Bbox Head pred took 64.24999237060547 ms\n",
      "\n",
      "Preprocessing took 58.129310607910156 ms\n",
      "Voxel Encoder took 0.8270740509033203 ms\n",
      "Middle Encoder/Imagify took 28.937816619873047 ms\n",
      "Backbone + FPN took 1.001596450805664 ms\n",
      "Bbox Head pred took 65.66667556762695 ms\n",
      "\n",
      "Preprocessing took 58.19582939147949 ms\n",
      "Voxel Encoder took 0.5245208740234375 ms\n",
      "Middle Encoder/Imagify took 32.82451629638672 ms\n",
      "Backbone + FPN took 1.3267993927001953 ms\n",
      "Bbox Head pred took 66.9107437133789 ms\n",
      "\n",
      "Preprocessing took 58.255910873413086 ms\n",
      "Voxel Encoder took 0.5269050598144531 ms\n",
      "Middle Encoder/Imagify took 27.70853042602539 ms\n",
      "Backbone + FPN took 1.0898113250732422 ms\n",
      "Bbox Head pred took 65.52720069885254 ms\n",
      "\n",
      "Preprocessing took 54.80360984802246 ms\n",
      "Voxel Encoder took 0.5292892456054688 ms\n",
      "Middle Encoder/Imagify took 21.691560745239258 ms\n",
      "Backbone + FPN took 1.0478496551513672 ms\n",
      "Bbox Head pred took 64.21923637390137 ms\n",
      "\n",
      "Preprocessing took 58.870792388916016 ms\n",
      "Voxel Encoder took 0.7109642028808594 ms\n",
      "Middle Encoder/Imagify took 37.56093978881836 ms\n",
      "Backbone + FPN took 1.0881423950195312 ms\n",
      "Bbox Head pred took 65.66596031188965 ms\n",
      "\n",
      "Preprocessing took 54.882049560546875 ms\n",
      "Voxel Encoder took 0.5247592926025391 ms\n",
      "Middle Encoder/Imagify took 23.505210876464844 ms\n",
      "Backbone + FPN took 1.1413097381591797 ms\n",
      "Bbox Head pred took 65.9172534942627 ms\n",
      "\n",
      "Preprocessing took 53.460121154785156 ms\n",
      "Voxel Encoder took 0.6191730499267578 ms\n",
      "Middle Encoder/Imagify took 28.11431884765625 ms\n",
      "Backbone + FPN took 1.0833740234375 ms\n",
      "Bbox Head pred took 65.71459770202637 ms\n",
      "\n",
      "Preprocessing took 59.20982360839844 ms\n",
      "Voxel Encoder took 0.5729198455810547 ms\n",
      "Middle Encoder/Imagify took 26.901721954345703 ms\n",
      "Backbone + FPN took 0.9894371032714844 ms\n",
      "Bbox Head pred took 66.00499153137207 ms\n",
      "\n",
      "Preprocessing took 58.34507942199707 ms\n",
      "Voxel Encoder took 0.8103847503662109 ms\n",
      "Middle Encoder/Imagify took 22.754669189453125 ms\n",
      "Backbone + FPN took 0.9980201721191406 ms\n",
      "Bbox Head pred took 65.0167465209961 ms\n",
      "\n",
      "Preprocessing took 58.305978775024414 ms\n",
      "Voxel Encoder took 0.5445480346679688 ms\n",
      "Middle Encoder/Imagify took 30.724287033081055 ms\n",
      "Backbone + FPN took 1.1608600616455078 ms\n",
      "Bbox Head pred took 66.31588935852051 ms\n",
      "\n",
      "Preprocessing took 55.65929412841797 ms\n",
      "Voxel Encoder took 82.38410949707031 ms\n",
      "Middle Encoder/Imagify took 26.733875274658203 ms\n",
      "Backbone + FPN took 1.039266586303711 ms\n",
      "Bbox Head pred took 64.42046165466309 ms\n",
      "\n",
      "Preprocessing took 47.975778579711914 ms\n",
      "Voxel Encoder took 0.5753040313720703 ms\n",
      "Middle Encoder/Imagify took 26.903867721557617 ms\n",
      "Backbone + FPN took 1.055002212524414 ms\n",
      "Bbox Head pred took 62.05630302429199 ms\n",
      "\n",
      "Preprocessing took 51.39303207397461 ms\n",
      "Voxel Encoder took 0.6108283996582031 ms\n",
      "Middle Encoder/Imagify took 30.27033805847168 ms\n",
      "Backbone + FPN took 1.0726451873779297 ms\n",
      "Bbox Head pred took 65.39726257324219 ms\n",
      "\n",
      "Preprocessing took 57.070016860961914 ms\n",
      "Voxel Encoder took 0.8420944213867188 ms\n",
      "Middle Encoder/Imagify took 27.323484420776367 ms\n",
      "Backbone + FPN took 1.0013580322265625 ms\n",
      "Bbox Head pred took 64.36491012573242 ms\n",
      "\n",
      "Preprocessing took 44.28744316101074 ms\n",
      "Voxel Encoder took 0.5259513854980469 ms\n",
      "Middle Encoder/Imagify took 29.08492088317871 ms\n",
      "Backbone + FPN took 0.9958744049072266 ms\n",
      "Bbox Head pred took 66.6036605834961 ms\n",
      "\n",
      "Preprocessing took 51.04541778564453 ms\n",
      "Voxel Encoder took 0.5300045013427734 ms\n",
      "Middle Encoder/Imagify took 29.7086238861084 ms\n",
      "Backbone + FPN took 1.0094642639160156 ms\n",
      "Bbox Head pred took 67.16108322143555 ms\n",
      "\n",
      "Preprocessing took 55.783748626708984 ms\n",
      "Voxel Encoder took 0.9074211120605469 ms\n",
      "Middle Encoder/Imagify took 33.43677520751953 ms\n",
      "Backbone + FPN took 1.0216236114501953 ms\n",
      "Bbox Head pred took 65.42134284973145 ms\n",
      "\n",
      "Preprocessing took 56.78439140319824 ms\n",
      "Voxel Encoder took 0.6585121154785156 ms\n",
      "Middle Encoder/Imagify took 33.19811820983887 ms\n",
      "Backbone + FPN took 1.3217926025390625 ms\n",
      "Bbox Head pred took 64.98241424560547 ms\n",
      "\n",
      "Preprocessing took 51.56373977661133 ms\n",
      "Voxel Encoder took 0.514984130859375 ms\n",
      "Middle Encoder/Imagify took 29.846906661987305 ms\n",
      "Backbone + FPN took 1.0411739349365234 ms\n",
      "Bbox Head pred took 64.9411678314209 ms\n",
      "\n",
      "Preprocessing took 54.40378189086914 ms\n",
      "Voxel Encoder took 0.5273818969726562 ms\n",
      "Middle Encoder/Imagify took 34.77787971496582 ms\n",
      "Backbone + FPN took 1.0809898376464844 ms\n",
      "Bbox Head pred took 66.0543441772461 ms\n",
      "\n",
      "Preprocessing took 59.279680252075195 ms\n",
      "Voxel Encoder took 0.865936279296875 ms\n",
      "Middle Encoder/Imagify took 29.557228088378906 ms\n",
      "Backbone + FPN took 1.0044574737548828 ms\n",
      "Bbox Head pred took 64.66054916381836 ms\n",
      "\n",
      "Preprocessing took 57.150840759277344 ms\n",
      "Voxel Encoder took 0.5013942718505859 ms\n",
      "Middle Encoder/Imagify took 32.7303409576416 ms\n",
      "Backbone + FPN took 1.2445449829101562 ms\n",
      "Bbox Head pred took 64.67247009277344 ms\n",
      "\n",
      "Preprocessing took 56.07151985168457 ms\n",
      "Voxel Encoder took 0.6797313690185547 ms\n",
      "Middle Encoder/Imagify took 25.728464126586914 ms\n",
      "Backbone + FPN took 1.2731552124023438 ms\n",
      "Bbox Head pred took 66.90168380737305 ms\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      2\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 3\u001b[0m \u001b[43minferencer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mpred_score_thr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime Taken \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m ms\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat((end\u001b[38;5;241m-\u001b[39mstart)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1000\u001b[39m))\n",
      "File \u001b[0;32m~/LidarObjDetection/mmdetection3d_dona/mmdetection3d/mmdet3d/apis/inferencers/base_3d_inferencer.py:214\u001b[0m, in \u001b[0;36mBase3DInferencer.__call__\u001b[0;34m(self, inputs, batch_size, return_datasamples, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m results_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m: [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvisualization\u001b[39m\u001b[38;5;124m'\u001b[39m: []}\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m (track(inputs, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInference\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    213\u001b[0m              \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshow_progress \u001b[38;5;28;01melse\u001b[39;00m inputs):\n\u001b[0;32m--> 214\u001b[0m     preds\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_kwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;66;03m# preds[1] is None for losses\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     visualization \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisualize(ori_inputs, preds,\n\u001b[1;32m    216\u001b[0m                                    \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvisualize_kwargs)\n\u001b[1;32m    217\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(preds, visualization,\n\u001b[1;32m    218\u001b[0m                                return_datasamples,\n\u001b[1;32m    219\u001b[0m                                \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_kwargs)\n",
      "File \u001b[0;32m~/venvs/pytorch_cuda123_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/pytorch_cuda123_env/lib/python3.10/site-packages/mmengine/infer/infer.py:296\u001b[0m, in \u001b[0;36mBaseInferencer.forward\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Union[\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mtuple\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    295\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Feed the inputs to the model.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/pytorch_cuda123_env/lib/python3.10/site-packages/mmengine/model/base_model/base_model.py:144\u001b[0m, in \u001b[0;36mBaseModel.test_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Union[\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    136\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"``BaseModel`` implements ``test_step`` the same as ``val_step``.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m        list: The predictions of given data.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_preprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_forward(data, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/venvs/pytorch_cuda123_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/pytorch_cuda123_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/LidarObjDetection/mmdetection3d_dona/mmdetection3d/mmdet3d/models/data_preprocessors/data_preprocessor.py:154\u001b[0m, in \u001b[0;36mDet3DDataPreprocessor.forward\u001b[0;34m(self, data, training)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m    153\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 154\u001b[0m     tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimple_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing took \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m ms\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat((end\u001b[38;5;241m-\u001b[39mstart)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1000\u001b[39m))\n",
      "File \u001b[0;32m~/LidarObjDetection/mmdetection3d_dona/mmdetection3d/mmdet3d/models/data_preprocessors/data_preprocessor.py:183\u001b[0m, in \u001b[0;36mDet3DDataPreprocessor.simple_process\u001b[0;34m(self, data, training)\u001b[0m\n\u001b[1;32m    180\u001b[0m     batch_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoints\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoints\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvoxel:\n\u001b[0;32m--> 183\u001b[0m         voxel_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvoxelize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpoints\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m         batch_inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoxels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m voxel_dict\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimgs\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs:\n",
      "File \u001b[0;32m~/venvs/pytorch_cuda123_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LidarObjDetection/mmdetection3d_dona/mmdetection3d/mmdet3d/models/data_preprocessors/data_preprocessor.py:372\u001b[0m, in \u001b[0;36mDet3DDataPreprocessor.voxelize\u001b[0;34m(self, points, data_samples)\u001b[0m\n\u001b[1;32m    370\u001b[0m voxels, coors, num_points, voxel_centers \u001b[38;5;241m=\u001b[39m [], [], [], []\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(points):\n\u001b[0;32m--> 372\u001b[0m     res_voxels, res_coors, res_num_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvoxel_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m     res_voxel_centers \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    374\u001b[0m         res_coors[:, [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m) \u001b[38;5;241m*\u001b[39m res_voxels\u001b[38;5;241m.\u001b[39mnew_tensor(\n\u001b[1;32m    375\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvoxel_layer\u001b[38;5;241m.\u001b[39mvoxel_size) \u001b[38;5;241m+\u001b[39m res_voxels\u001b[38;5;241m.\u001b[39mnew_tensor(\n\u001b[1;32m    376\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvoxel_layer\u001b[38;5;241m.\u001b[39mpoint_cloud_range[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m    377\u001b[0m     res_coors \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(res_coors, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m, value\u001b[38;5;241m=\u001b[39mi)\n",
      "File \u001b[0;32m~/venvs/pytorch_cuda123_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/pytorch_cuda123_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/LidarObjDetection/mmdetection3d_dona/mmdetection3d/mmdet3d/models/data_preprocessors/voxelize.py:170\u001b[0m, in \u001b[0;36mVoxelizationByGridShape.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     max_voxels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_voxels[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvoxelization\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvoxel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoint_cloud_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_num_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_voxels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/pytorch_cuda123_env/lib/python3.10/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/LidarObjDetection/mmdetection3d_dona/mmdetection3d/mmdet3d/models/data_preprocessors/voxelize.py:78\u001b[0m, in \u001b[0;36m_Voxelization.forward\u001b[0;34m(ctx, points, voxel_size, coors_range, max_points, max_voxels, deterministic)\u001b[0m\n\u001b[1;32m     75\u001b[0m num_points_per_voxel \u001b[38;5;241m=\u001b[39m points\u001b[38;5;241m.\u001b[39mnew_zeros(\n\u001b[1;32m     76\u001b[0m     size\u001b[38;5;241m=\u001b[39m(max_voxels, ), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint)\n\u001b[1;32m     77\u001b[0m voxel_num \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(size\u001b[38;5;241m=\u001b[39m(), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m---> 78\u001b[0m \u001b[43mext_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhard_voxelize_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvoxel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoors_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvoxels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_points_per_voxel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvoxel_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_points\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_voxels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_voxels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mNDim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# select the valid voxels\u001b[39;00m\n\u001b[1;32m     91\u001b[0m voxels_out \u001b[38;5;241m=\u001b[39m voxels[:voxel_num]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "inferencer(inputs, show=False,pred_score_thr=0.3,batch_size=2)\n",
    "end = time.time()\n",
    "print(\"Time Taken {} ms\".format((end-start)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Size!\n",
      "Preprocessing took 8.751153945922852 ms\n"
     ]
    }
   ],
   "source": [
    "from mmdet3d.datasets.transforms.loading import (LoadAnnotations3D,\n",
    "                                                 LoadPointsFromFile)\n",
    "from mmdet3d.models.data_preprocessors.data_preprocessor import \\\n",
    "    Det3DDataPreprocessor\n",
    "from mmdet3d.datasets.transforms.formating import Pack3DDetInputs\n",
    "import os \n",
    "\n",
    "model_inputs = {}\n",
    "model_inputs[\"inputs\"] = {}\n",
    "model_inputs[\"inputs\"][\"points\"] = []\n",
    "max = 2\n",
    "\n",
    "files = []\n",
    "if not os.path.isdir(inputs[\"points\"]):\n",
    "    files.append(inputs[\"points\"])\n",
    "else:\n",
    "    files = [inputs[\"points\"] + s for s in os.listdir(inputs[\"points\"])]\n",
    "for i,sample_path in enumerate(files):\n",
    "    inp = {}\n",
    "    inp[\"lidar_points\"] = {}\n",
    "    inp[\"lidar_points\"][\"lidar_path\"] = sample_path\n",
    "\n",
    "\n",
    "    loader = LoadPointsFromFile(coord_type='LIDAR',load_dim=4,use_dim=4)\n",
    "    packer = Pack3DDetInputs(keys=['points'])\n",
    "\n",
    "    voxel_size = [0.16, 0.16, 4]\n",
    "    preprocessor = Det3DDataPreprocessor(\n",
    "        voxel=True,\n",
    "        voxel_layer=dict(\n",
    "            max_num_points=32,  # max_points_per_voxel\n",
    "            point_cloud_range=[0, -39.68, -3, 69.12, 39.68, 1],\n",
    "            voxel_size=voxel_size,\n",
    "            max_voxels=(16000, 40000)))\n",
    "\n",
    "    input = loader(inp)\n",
    "    input = packer(input)\n",
    "    # input[\"inputs\"][\"points\"] = input[\"inputs\"][\"points\"].cuda()\n",
    "    model_inputs[\"inputs\"][\"points\"].append(input[\"inputs\"][\"points\"])\n",
    "    if i == max-1:\n",
    "        print(\"Max Size!\")\n",
    "        break\n",
    "prep_input = preprocessor(model_inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_input[\"inputs\"][\"voxels\"][\"voxels\"] = prep_input[\"inputs\"][\"voxels\"][\"voxels\"].cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = inferencer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([23358, 32, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_input[\"inputs\"][\"voxels\"][\"voxels\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voxel Encoder took 7.731199264526367 ms\n",
      "Middle Encoder/Imagify took 25.522470474243164 ms\n",
      "Backbone + FPN took 16.983985900878906 ms\n",
      "Bbox Head forward took 0.9174346923828125 ms\n",
      "\n",
      "Time: 53.93028259277344 ms\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import torch\n",
    "s = time.time()\n",
    "res = model(prep_input[\"inputs\"],prep_input[\"data_samples\"], mode=\"tensor\")\n",
    "print(\"Time: {} ms\".format((time.time()-s)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = inferencer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on GPU\n"
     ]
    }
   ],
   "source": [
    "# Check the device of the model\n",
    "device = next(inferencer.model.parameters()).device\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(\"Model is on GPU\")\n",
    "else:\n",
    "    print(\"Model is on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 4814868\n",
      "Parameter: backbone.blocks.0.0.weight, Size: torch.Size([64, 64, 3, 3])\n",
      "Parameter: backbone.blocks.0.1.weight, Size: torch.Size([64])\n",
      "Parameter: backbone.blocks.0.1.bias, Size: torch.Size([64])\n",
      "Parameter: backbone.blocks.0.3.weight, Size: torch.Size([64, 64, 3, 3])\n",
      "Parameter: backbone.blocks.0.4.weight, Size: torch.Size([64])\n",
      "Parameter: backbone.blocks.0.4.bias, Size: torch.Size([64])\n",
      "Parameter: backbone.blocks.0.6.weight, Size: torch.Size([64, 64, 3, 3])\n",
      "Parameter: backbone.blocks.0.7.weight, Size: torch.Size([64])\n",
      "Parameter: backbone.blocks.0.7.bias, Size: torch.Size([64])\n",
      "Parameter: backbone.blocks.0.9.weight, Size: torch.Size([64, 64, 3, 3])\n",
      "Parameter: backbone.blocks.0.10.weight, Size: torch.Size([64])\n",
      "Parameter: backbone.blocks.0.10.bias, Size: torch.Size([64])\n",
      "Parameter: backbone.blocks.1.0.weight, Size: torch.Size([128, 64, 3, 3])\n",
      "Parameter: backbone.blocks.1.1.weight, Size: torch.Size([128])\n",
      "Parameter: backbone.blocks.1.1.bias, Size: torch.Size([128])\n",
      "Parameter: backbone.blocks.1.3.weight, Size: torch.Size([128, 128, 3, 3])\n",
      "Parameter: backbone.blocks.1.4.weight, Size: torch.Size([128])\n",
      "Parameter: backbone.blocks.1.4.bias, Size: torch.Size([128])\n",
      "Parameter: backbone.blocks.1.6.weight, Size: torch.Size([128, 128, 3, 3])\n",
      "Parameter: backbone.blocks.1.7.weight, Size: torch.Size([128])\n",
      "Parameter: backbone.blocks.1.7.bias, Size: torch.Size([128])\n",
      "Parameter: backbone.blocks.1.9.weight, Size: torch.Size([128, 128, 3, 3])\n",
      "Parameter: backbone.blocks.1.10.weight, Size: torch.Size([128])\n",
      "Parameter: backbone.blocks.1.10.bias, Size: torch.Size([128])\n",
      "Parameter: backbone.blocks.1.12.weight, Size: torch.Size([128, 128, 3, 3])\n",
      "Parameter: backbone.blocks.1.13.weight, Size: torch.Size([128])\n",
      "Parameter: backbone.blocks.1.13.bias, Size: torch.Size([128])\n",
      "Parameter: backbone.blocks.1.15.weight, Size: torch.Size([128, 128, 3, 3])\n",
      "Parameter: backbone.blocks.1.16.weight, Size: torch.Size([128])\n",
      "Parameter: backbone.blocks.1.16.bias, Size: torch.Size([128])\n",
      "Parameter: backbone.blocks.2.0.weight, Size: torch.Size([256, 128, 3, 3])\n",
      "Parameter: backbone.blocks.2.1.weight, Size: torch.Size([256])\n",
      "Parameter: backbone.blocks.2.1.bias, Size: torch.Size([256])\n",
      "Parameter: backbone.blocks.2.3.weight, Size: torch.Size([256, 256, 3, 3])\n",
      "Parameter: backbone.blocks.2.4.weight, Size: torch.Size([256])\n",
      "Parameter: backbone.blocks.2.4.bias, Size: torch.Size([256])\n",
      "Parameter: backbone.blocks.2.6.weight, Size: torch.Size([256, 256, 3, 3])\n",
      "Parameter: backbone.blocks.2.7.weight, Size: torch.Size([256])\n",
      "Parameter: backbone.blocks.2.7.bias, Size: torch.Size([256])\n",
      "Parameter: backbone.blocks.2.9.weight, Size: torch.Size([256, 256, 3, 3])\n",
      "Parameter: backbone.blocks.2.10.weight, Size: torch.Size([256])\n",
      "Parameter: backbone.blocks.2.10.bias, Size: torch.Size([256])\n",
      "Parameter: backbone.blocks.2.12.weight, Size: torch.Size([256, 256, 3, 3])\n",
      "Parameter: backbone.blocks.2.13.weight, Size: torch.Size([256])\n",
      "Parameter: backbone.blocks.2.13.bias, Size: torch.Size([256])\n",
      "Parameter: backbone.blocks.2.15.weight, Size: torch.Size([256, 256, 3, 3])\n",
      "Parameter: backbone.blocks.2.16.weight, Size: torch.Size([256])\n",
      "Parameter: backbone.blocks.2.16.bias, Size: torch.Size([256])\n",
      "Parameter: neck.deblocks.0.0.weight, Size: torch.Size([64, 128, 1, 1])\n",
      "Parameter: neck.deblocks.0.1.weight, Size: torch.Size([128])\n",
      "Parameter: neck.deblocks.0.1.bias, Size: torch.Size([128])\n",
      "Parameter: neck.deblocks.1.0.weight, Size: torch.Size([128, 128, 2, 2])\n",
      "Parameter: neck.deblocks.1.1.weight, Size: torch.Size([128])\n",
      "Parameter: neck.deblocks.1.1.bias, Size: torch.Size([128])\n",
      "Parameter: neck.deblocks.2.0.weight, Size: torch.Size([256, 128, 4, 4])\n",
      "Parameter: neck.deblocks.2.1.weight, Size: torch.Size([128])\n",
      "Parameter: neck.deblocks.2.1.bias, Size: torch.Size([128])\n",
      "Parameter: bbox_head.conv_cls.weight, Size: torch.Size([2, 384, 1, 1])\n",
      "Parameter: bbox_head.conv_cls.bias, Size: torch.Size([2])\n",
      "Parameter: bbox_head.conv_reg.weight, Size: torch.Size([14, 384, 1, 1])\n",
      "Parameter: bbox_head.conv_reg.bias, Size: torch.Size([14])\n",
      "Parameter: bbox_head.conv_dir_cls.weight, Size: torch.Size([4, 384, 1, 1])\n",
      "Parameter: bbox_head.conv_dir_cls.bias, Size: torch.Size([4])\n",
      "Parameter: voxel_encoder.pfn_layers.0.norm.weight, Size: torch.Size([64])\n",
      "Parameter: voxel_encoder.pfn_layers.0.norm.bias, Size: torch.Size([64])\n",
      "Parameter: voxel_encoder.pfn_layers.0.linear.weight, Size: torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# Count the number of parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "\n",
    "# Access and print model parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter: {name}, Size: {param.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(inputs,batch_size=1):\n",
    "    model_inputs = {}\n",
    "    model_inputs[\"inputs\"] = {}\n",
    "    model_inputs[\"inputs\"][\"points\"] = []\n",
    "    max = batch_size\n",
    "\n",
    "    files = []\n",
    "    if not os.path.isdir(inputs[\"points\"]):\n",
    "        files.append(inputs[\"points\"])\n",
    "    else:\n",
    "        files = [inputs[\"points\"] + s for s in os.listdir(inputs[\"points\"])]\n",
    "    for i,sample_path in enumerate(files):\n",
    "        inp = {}\n",
    "        inp[\"lidar_points\"] = {}\n",
    "        inp[\"lidar_points\"][\"lidar_path\"] = sample_path\n",
    "\n",
    "\n",
    "        loader = LoadPointsFromFile(coord_type='LIDAR',load_dim=4,use_dim=4)\n",
    "        packer = Pack3DDetInputs(keys=['points'])\n",
    "\n",
    "        voxel_size = [0.16, 0.16, 4]\n",
    "        preprocessor = Det3DDataPreprocessor(\n",
    "            voxel=True,\n",
    "            voxel_layer=dict(\n",
    "                max_num_points=32,  # max_points_per_voxel\n",
    "                point_cloud_range=[0, -39.68, -3, 69.12, 39.68, 1],\n",
    "                voxel_size=voxel_size,\n",
    "                max_voxels=(16000, 40000)))\n",
    "\n",
    "        input = loader(inp)\n",
    "        input = packer(input)\n",
    "        # input[\"inputs\"][\"points\"] = input[\"inputs\"][\"points\"].cuda()\n",
    "        model_inputs[\"inputs\"][\"points\"].append(input[\"inputs\"][\"points\"])\n",
    "        if i == max-1:\n",
    "            print(\"Batch Size!\")\n",
    "            break\n",
    "    prep_input = preprocessor(model_inputs)\n",
    "    prep_input[\"inputs\"][\"voxels\"][\"voxels\"] = prep_input[\"inputs\"][\"voxels\"][\"voxels\"].cuda()\n",
    "    return prep_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m inputs \u001b[38;5;241m=\u001b[39m prepare_input({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m: pcl_path}, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Evaluate on validation data\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     14\u001b[0m     val_output \u001b[38;5;241m=\u001b[39m model(inputs,mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from mmdet3d.datasets.transforms.loading import (LoadAnnotations3D,\n",
    "                                                LoadPointsFromFile)\n",
    "from mmdet3d.models.data_preprocessors.data_preprocessor import \\\n",
    "    Det3DDataPreprocessor\n",
    "from mmdet3d.datasets.transforms.formating import Pack3DDetInputs\n",
    "\n",
    "pcl_path = \"../data/DonaSet/testing/velodyne/\"\n",
    "inputs = prepare_input({\"points\": pcl_path}, batch_size=3)[\"inputs\"]\n",
    "# Evaluate on validation data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_output = model(inputs,mode=\"tensor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model Operator Set Version: 11\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model_path = \"end2end2.onnx\"\n",
    "m = onnx.load(onnx_model_path)\n",
    "opset_version = m.opset_import[0].version if m.opset_import else None\n",
    "\n",
    "print(\"ONNX Model Operator Set Version:\", opset_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Runtime Version: 1.15.1\n",
      "GPU\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_version = onnxruntime.__version__\n",
    "print(\"ONNX Runtime Version:\", ort_version)\n",
    "\n",
    "print( onnxruntime.get_device()  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This ORT build has ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'] enabled. Since ORT 1.9, you are required to explicitly set the providers parameter when instantiating InferenceSession. For example, onnxruntime.InferenceSession(..., providers=['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'], ...)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m onnx\u001b[38;5;241m.\u001b[39mchecker\u001b[38;5;241m.\u001b[39mcheck_model(onnx_model)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# x, y = test_data[0][0], test_data[0][1]\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m ort_sess \u001b[38;5;241m=\u001b[39m \u001b[43mort\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m onnx_inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvoxels\u001b[39m\u001b[38;5;124m'\u001b[39m: inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvoxels\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvoxels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoors\u001b[39m\u001b[38;5;124m'\u001b[39m:inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvoxels\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoors\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(),\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_points\u001b[39m\u001b[38;5;124m'\u001b[39m:inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvoxels\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_points\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()}\n\u001b[1;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m ort_sess\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28;01mNone\u001b[39;00m, onnx_inputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:396\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m fallback_error \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;66;03m# Fallback is disabled. Raise the original error.\u001b[39;00m\n\u001b[0;32m--> 396\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:383\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m disabled_optimizers \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 383\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_inference_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:415\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m providers \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(available_providers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable_fallback()\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis ORT build has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_providers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m enabled. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSince ORT 1.9, you are required to explicitly set \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe providers parameter when instantiating InferenceSession. For example, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnxruntime.InferenceSession(..., providers=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_providers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, ...)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    420\u001b[0m     )\n\u001b[1;32m    422\u001b[0m session_options \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess_options \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess_options \u001b[38;5;28;01melse\u001b[39;00m C\u001b[38;5;241m.\u001b[39mget_default_session_options()\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_path:\n",
      "\u001b[0;31mValueError\u001b[0m: This ORT build has ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'] enabled. Since ORT 1.9, you are required to explicitly set the providers parameter when instantiating InferenceSession. For example, onnxruntime.InferenceSession(..., providers=['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'], ...)"
     ]
    }
   ],
   "source": [
    "\n",
    "import onnx\n",
    "import onnxruntime\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "model_path = 'end2end2.onnx'\n",
    "onnx_model = onnx.load(model_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "\n",
    "# x, y = test_data[0][0], test_data[0][1]\n",
    "ort_sess = ort.InferenceSession(model_path)\n",
    "onnx_inputs = {'voxels': inputs[\"voxels\"][\"voxels\"].cpu().numpy(), 'coors':inputs[\"voxels\"][\"coors\"].cpu().numpy(),'num_points':inputs[\"voxels\"][\"num_points\"].cpu().numpy()}\n",
    "outputs = ort_sess.run(None, onnx_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 248, 216]), (1, 2, 248, 216))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_output[0][0].shape,outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-20 21:08:29.326350419 [W:onnxruntime:Default, tensorrt_execution_provider.h:75 log] [2024-02-20 21:08:29 WARNING] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "2024-02-20 21:08:29.326376271 [W:onnxruntime:Default, tensorrt_execution_provider.h:75 log] [2024-02-20 21:08:29 WARNING] onnx2trt_utils.cpp:400: One or more weights outside the range of INT32 was clamped\n",
      "2024-02-20 21:08:29.407009799 [W:onnxruntime:Default, tensorrt_execution_provider.h:75 log] [2024-02-20 21:08:29 WARNING] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "2024-02-20 21:08:29.407033456 [W:onnxruntime:Default, tensorrt_execution_provider.h:75 log] [2024-02-20 21:08:29 WARNING] onnx2trt_utils.cpp:400: One or more weights outside the range of INT32 was clamped\n"
     ]
    }
   ],
   "source": [
    "# options = ort.SessionOptions()\n",
    "# options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL\n",
    "# options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "# # Specify the target device as CUDA if available\n",
    "# # options.intra_op_num_threads = 1\n",
    "# options.execution_mode = ort.ExecutionMode.ORT_SEQUENTIAL\n",
    "# options.add_session_config_entry('session.device_id', '0')  # Set the GPU device ID\n",
    "\n",
    "# Create the InferenceSession\n",
    "ort_sess = ort.InferenceSession(model_path,providers=['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_inputs = {'voxels': inputs[\"voxels\"][\"voxels\"].cpu().numpy(), 'coors':inputs[\"voxels\"][\"coors\"].cpu().numpy(),'num_points':inputs[\"voxels\"][\"num_points\"].cpu().numpy()}\n",
    "outputs = ort_sess.run(None, onnx_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 26.1385440826416 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "s = time.time()\n",
    "outputs = ort_sess.run(None, onnx_inputs)\n",
    "print(\"Time: {} ms\".format((time.time()-s)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 248, 216)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riccardo/venvs/pytorch_cuda123_env/lib/python3.10/site-packages/torch/onnx/_internal/exporter.py:137: UserWarning: torch.onnx.dynamo_export only implements opset version 18 for now. If you need to use a different opset version, please register them with register_custom_op.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "torch_model = MyModel()\n",
    "torch_input = torch.randn(1, 1, 32, 32)\n",
    "onnx_program = torch.onnx.dynamo_export(torch_model, torch_input)\n",
    "onnx_program.save(\"my_image_classifier.onnx\")\n",
    "\n",
    "import onnx\n",
    "onnx_model = onnx.load(\"my_image_classifier.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input length: 1\n",
      "Sample input: (tensor([[[[ 0.4290,  0.0823,  0.3877,  ...,  0.2530, -1.5004,  0.2132],\n",
      "          [ 0.0971, -1.2425,  1.5864,  ..., -0.5080,  0.2800,  0.0344],\n",
      "          [-0.2636,  0.6221,  0.5165,  ...,  0.2748,  0.5340, -0.6011],\n",
      "          ...,\n",
      "          [ 0.3662, -1.2284,  2.0043,  ...,  0.9872,  0.5133,  0.5443],\n",
      "          [-1.1936,  1.5664,  0.0353,  ...,  0.4518,  0.2531,  0.2016],\n",
      "          [ 0.2531,  2.8630, -0.9375,  ..., -0.1628, -1.0180, -0.6954]]]]),)\n"
     ]
    },
    {
     "ename": "Fail",
     "evalue": "[ONNXRuntimeError] : 1 : FAIL : Load model from ./my_image_classifier.onnx failed:/onnxruntime_src/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map<std::__cxx11::basic_string<char>, int>&, const onnxruntime::logging::Logger&, bool, const string&, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 18 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 17.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFail\u001b[0m                                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(onnx_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample input: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00monnx_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m ort_session \u001b[38;5;241m=\u001b[39m \u001b[43monnxruntime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./my_image_classifier.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproviders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCPUExecutionProvider\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_numpy\u001b[39m(tensor):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;28;01melse\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/venvs/pytorch_cuda123_env/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:347\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m disabled_optimizers \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_inference_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "File \u001b[0;32m~/venvs/pytorch_cuda123_env/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:384\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    382\u001b[0m session_options \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess_options \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess_options \u001b[38;5;28;01melse\u001b[39;00m C\u001b[38;5;241m.\u001b[39mget_default_session_options()\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_path:\n\u001b[0;32m--> 384\u001b[0m     sess \u001b[38;5;241m=\u001b[39m \u001b[43mC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_config_from_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     sess \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mInferenceSession(session_options, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_bytes, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_config_from_model)\n",
      "\u001b[0;31mFail\u001b[0m: [ONNXRuntimeError] : 1 : FAIL : Load model from ./my_image_classifier.onnx failed:/onnxruntime_src/onnxruntime/core/graph/model_load_utils.h:57 void onnxruntime::model_load_utils::ValidateOpsetForDomain(const std::unordered_map<std::__cxx11::basic_string<char>, int>&, const onnxruntime::logging::Logger&, bool, const string&, int) ONNX Runtime only *guarantees* support for models stamped with official released onnx opset versions. Opset 18 is under development and support for this is limited. The operator schemas and or other functionality may change before next ONNX release and in this case ONNX Runtime will not guarantee backward compatibility. Current official support for domain ai.onnx is till opset 17.\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "\n",
    "onnx_input = onnx_program.adapt_torch_inputs_to_onnx(torch_input)\n",
    "print(f\"Input length: {len(onnx_input)}\")\n",
    "print(f\"Sample input: {onnx_input}\")\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(\"./my_image_classifier.onnx\", providers=['CPUExecutionProvider'])\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "onnxruntime_input = {k.name: to_numpy(v) for k, v in zip(ort_session.get_inputs(), onnx_input)}\n",
    "\n",
    "onnxruntime_outputs = ort_session.run(None, onnxruntime_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 17.639317  , -13.64231   ,   0.98981375,   0.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "path = '../data/DonaSet/testing/velodyne/000001.bin'\n",
    "\n",
    "fields = 4\n",
    "# Read the binary file into a NumPy array\n",
    "data = np.fromfile(path, dtype=np.float32)\n",
    "\n",
    "# Reshape the 1D array to a 2D array with 4 columns\n",
    "data = data.reshape(-1, 4)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmdet3d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstructures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m  Det3DDataSample\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming you have a single sample as a NumPy array or list\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Convert the sample to a PyTorch tensor\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mdata\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Add batch dimension (if needed)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m input_tensor  \u001b[38;5;66;03m# For models expecting a batch dimension\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from mmdet3d.structures import  Det3DDataSample\n",
    "# Assuming you have a single sample as a NumPy array or list\n",
    "\n",
    "# Convert the sample to a PyTorch tensor\n",
    "input_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "\n",
    "# Add batch dimension (if needed)\n",
    "input_tensor = input_tensor  # For models expecting a batch dimension\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "model_input = {}\n",
    "model_input[\"inputs\"] = {}\n",
    "model_input[\"inputs\"][\"points\"] = [input_tensor]\n",
    "data_sample = Det3DDataSample()\n",
    "model_input[\"data_samples\"] = [data_sample]\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    output = model.test_step(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: torch.Size([64, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Print the input size of the first layer\n",
    "input_size = next(model.parameters()).size()[1:]\n",
    "print(\"Input size:\", input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riccardo/LidarObjDetection/mmdetection3d_dona/mmdetection3d/mmdet3d/models/dense_heads/anchor3d_head.py:94: UserWarning: dir_offset and dir_limit_offset will be depressed and be incorporated into box coder in the future\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'assigner'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmmdet3d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DATASETS, MODELS\n\u001b[0;32m----> 2\u001b[0m \u001b[43mMODELS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/pytorch_cuda123_env/lib/python3.10/site-packages/mmengine/registry/registry.py:570\u001b[0m, in \u001b[0;36mRegistry.build\u001b[0;34m(self, cfg, *args, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg: \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    549\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build an instance.\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \n\u001b[1;32m    551\u001b[0m \u001b[38;5;124;03m    Build an instance by calling :attr:`build_func`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03m        >>> model = MODELS.build(cfg)\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregistry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/pytorch_cuda123_env/lib/python3.10/site-packages/mmengine/registry/build_functions.py:232\u001b[0m, in \u001b[0;36mbuild_model_from_cfg\u001b[0;34m(cfg, registry, default_args)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Sequential(\u001b[38;5;241m*\u001b[39mmodules)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_from_cfg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregistry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/pytorch_cuda123_env/lib/python3.10/site-packages/mmengine/registry/build_functions.py:121\u001b[0m, in \u001b[0;36mbuild_from_cfg\u001b[0;34m(cfg, registry, default_args)\u001b[0m\n\u001b[1;32m    119\u001b[0m     obj \u001b[38;5;241m=\u001b[39m obj_cls\u001b[38;5;241m.\u001b[39mget_instance(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (inspect\u001b[38;5;241m.\u001b[39misclass(obj_cls) \u001b[38;5;129;01mor\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misfunction(obj_cls)\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mismethod(obj_cls)):\n\u001b[1;32m    125\u001b[0m     print_log(\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_cls\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` instance is built from \u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# type: ignore # noqa: E501\u001b[39;00m\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregistry, and its implementation can be found in \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_cls\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurrent\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    130\u001b[0m         level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mDEBUG)\n",
      "File \u001b[0;32m~/LidarObjDetection/mmdetection3d_dona/mmdetection3d/mmdet3d/models/detectors/voxelnet.py:25\u001b[0m, in \u001b[0;36mVoxelNet.__init__\u001b[0;34m(self, voxel_encoder, middle_encoder, backbone, neck, bbox_head, train_cfg, test_cfg, data_preprocessor, init_cfg)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     16\u001b[0m              voxel_encoder: ConfigType,\n\u001b[1;32m     17\u001b[0m              middle_encoder: ConfigType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m              data_preprocessor: OptConfigType \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     24\u001b[0m              init_cfg: OptMultiConfig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackbone\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneck\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbbox_head\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbbox_head\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_preprocessor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_preprocessor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvoxel_encoder \u001b[38;5;241m=\u001b[39m MODELS\u001b[38;5;241m.\u001b[39mbuild(voxel_encoder)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmiddle_encoder \u001b[38;5;241m=\u001b[39m MODELS\u001b[38;5;241m.\u001b[39mbuild(middle_encoder)\n",
      "File \u001b[0;32m~/LidarObjDetection/mmdetection3d_dona/mmdetection3d/mmdet3d/models/detectors/single_stage.py:52\u001b[0m, in \u001b[0;36mSingleStage3DDetector.__init__\u001b[0;34m(self, backbone, neck, bbox_head, train_cfg, test_cfg, data_preprocessor, init_cfg)\u001b[0m\n\u001b[1;32m     50\u001b[0m bbox_head\u001b[38;5;241m.\u001b[39mupdate(train_cfg\u001b[38;5;241m=\u001b[39mtrain_cfg)\n\u001b[1;32m     51\u001b[0m bbox_head\u001b[38;5;241m.\u001b[39mupdate(test_cfg\u001b[38;5;241m=\u001b[39mtest_cfg)\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbbox_head \u001b[38;5;241m=\u001b[39m \u001b[43mMODELS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbbox_head\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_cfg \u001b[38;5;241m=\u001b[39m train_cfg\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_cfg \u001b[38;5;241m=\u001b[39m test_cfg\n",
      "File \u001b[0;32m~/venvs/pytorch_cuda123_env/lib/python3.10/site-packages/mmengine/registry/registry.py:570\u001b[0m, in \u001b[0;36mRegistry.build\u001b[0;34m(self, cfg, *args, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg: \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    549\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build an instance.\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \n\u001b[1;32m    551\u001b[0m \u001b[38;5;124;03m    Build an instance by calling :attr:`build_func`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03m        >>> model = MODELS.build(cfg)\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregistry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/pytorch_cuda123_env/lib/python3.10/site-packages/mmengine/registry/build_functions.py:232\u001b[0m, in \u001b[0;36mbuild_model_from_cfg\u001b[0;34m(cfg, registry, default_args)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Sequential(\u001b[38;5;241m*\u001b[39mmodules)\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_from_cfg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregistry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/pytorch_cuda123_env/lib/python3.10/site-packages/mmengine/registry/build_functions.py:121\u001b[0m, in \u001b[0;36mbuild_from_cfg\u001b[0;34m(cfg, registry, default_args)\u001b[0m\n\u001b[1;32m    119\u001b[0m     obj \u001b[38;5;241m=\u001b[39m obj_cls\u001b[38;5;241m.\u001b[39mget_instance(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 121\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (inspect\u001b[38;5;241m.\u001b[39misclass(obj_cls) \u001b[38;5;129;01mor\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misfunction(obj_cls)\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mismethod(obj_cls)):\n\u001b[1;32m    125\u001b[0m     print_log(\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_cls\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` instance is built from \u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# type: ignore # noqa: E501\u001b[39;00m\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregistry, and its implementation can be found in \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_cls\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurrent\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    130\u001b[0m         level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mDEBUG)\n",
      "File \u001b[0;32m~/LidarObjDetection/mmdetection3d_dona/mmdetection3d/mmdet3d/models/dense_heads/anchor3d_head.py:118\u001b[0m, in \u001b[0;36mAnchor3DHead.__init__\u001b[0;34m(self, num_classes, in_channels, feat_channels, use_direction_classifier, anchor_generator, assigner_per_size, assign_per_class, diff_rad_by_sin, dir_offset, dir_limit_offset, bbox_coder, loss_cls, loss_bbox, loss_dir, train_cfg, test_cfg, init_cfg)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_dir \u001b[38;5;241m=\u001b[39m MODELS\u001b[38;5;241m.\u001b[39mbuild(loss_dir)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_layers()\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_assigner_sampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m init_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_cfg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNormal\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    123\u001b[0m         layer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConv2d\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    124\u001b[0m         std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m    125\u001b[0m         override\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    126\u001b[0m             \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNormal\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconv_cls\u001b[39m\u001b[38;5;124m'\u001b[39m, std\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, bias_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m))\n",
      "File \u001b[0;32m~/LidarObjDetection/mmdetection3d_dona/mmdetection3d/mmdet3d/models/dense_heads/anchor3d_head.py:137\u001b[0m, in \u001b[0;36mAnchor3DHead._init_assigner_sampler\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbbox_sampler \u001b[38;5;241m=\u001b[39m PseudoSampler()\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massigner\u001b[49m, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbbox_assigner \u001b[38;5;241m=\u001b[39m TASK_UTILS\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_cfg\u001b[38;5;241m.\u001b[39massigner)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_cfg\u001b[38;5;241m.\u001b[39massigner, \u001b[38;5;28mlist\u001b[39m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'assigner'"
     ]
    }
   ],
   "source": [
    "from mmdet3d.registry import DATASETS, MODELS\n",
    "MODELS.build(mod)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a0c343fece975dd89087e8c2194dd4d3db28d7000f1b32ed9ed9d584dd54dbbe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
