{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append(\"/home/riccardo/mmdetection3d\")\n",
    "# sys.path.append(\"/root/workspace/mmdeploy\")\n",
    "from mmdet3d.apis import LidarDet3DInferencer\n",
    "from mmdeploy.apis.inference import inference_model\n",
    "from mmdeploy.apis.inference import get_model\n",
    "import os\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "from mmdet3d.datasets.transforms.loading import (LoadAnnotations3D,\n",
    "                                                LoadPointsFromFile)\n",
    "from mmdet3d.models.data_preprocessors.data_preprocessor import \\\n",
    "    Det3DDataPreprocessor\n",
    "import onnxruntime as ort\n",
    "from mmdet3d.datasets.transforms.formating import Pack3DDetInputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize inferencer \n",
    "# execute >> pip install -v -e . in mmdetection3d folder \n",
    "# CHANGE THE center_mode in loca_visualizer accordingly (KITTI -> center_mode = 'lidar_bottom', CUSTOM -> center_mode = whatever)\n",
    "\n",
    "# inferencer = LidarDet3DInferencer('pointpillars_hv_secfpn_8xb6-160e_kitti-3d-car')\n",
    "# inferencer = LidarDet3DInferencer('pointpillars_donaset_container-car')\n",
    "# inferencer = LidarDet3DInferencer('pointpillars_kittilidar-car') \n",
    "# inferencer = LidarDet3DInferencer(\"pointpillars_hv_secfpn_8xb6-160e_lidar_vegas_iris-3d-car\",scope=\"mmdet3d\")\n",
    "# inferencer = LidarDet3DInferencer(\"pointpillars_lidar_vegas_iris-car_360\",scope=\"mmdet3d\")\n",
    "# inferencer = LidarDet3DInferencer(\"pointpillars_hydris_car\",scope=\"mmdet3d\")\n",
    "inferencer = LidarDet3DInferencer(\"pointpillars_hydra_car\",scope=\"mmdet3d\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"../data/Hydra/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def copy_bin_files(source_txt, bin_folder_path,destination_folder):\n",
    "\n",
    "\n",
    "  # Check if source text file exists\n",
    "  if not os.path.exists(source_txt):\n",
    "    print(f\"Error: Source text file '{source_txt}' does not exist.\")\n",
    "    return\n",
    "\n",
    "  # Check if destination folder exists, create it if not\n",
    "  if not os.path.exists(destination_folder):\n",
    "    try:\n",
    "      os.makedirs(destination_folder)\n",
    "      print(f\"Destination folder '{destination_folder}' created.\")\n",
    "    except OSError as e:\n",
    "      print(f\"Error creating destination folder: {e}\")\n",
    "      return\n",
    "\n",
    "  # Open the source text file\n",
    "  try:\n",
    "    with open(source_txt, 'r') as file:\n",
    "      lines = file.readlines()\n",
    "  except OSError as e:\n",
    "    print(f\"Error opening source text file: {e}\")\n",
    "    return\n",
    "\n",
    "  # Process each line in the text file\n",
    "  for line in lines:\n",
    "    # Remove leading/trailing whitespace and newline character\n",
    "    filename = line.strip() +\".bin\"\n",
    "\n",
    "    # Check if the filename ends with .bin\n",
    "    if not filename.endswith('.bin'):\n",
    "      print(f\"Warning: Skipping non-binary file '{filename}'.\")\n",
    "      continue\n",
    "\n",
    "    # Construct source and destination paths\n",
    "    source_path = os.path.join(bin_folder_path, filename)\n",
    "    dest_path = os.path.join(destination_folder, filename)\n",
    "\n",
    "    # Check if source file exists\n",
    "    if not os.path.exists(source_path):\n",
    "      print(f\"Warning: Source file '{source_path}' does not exist. Skipping...\")\n",
    "      continue\n",
    "\n",
    "    # Copy the file\n",
    "    try:\n",
    "      with open(source_path, 'rb') as source, open(dest_path, 'wb') as destination:\n",
    "        for chunk in iter(lambda: source.read(1024), b''):\n",
    "          destination.write(chunk)\n",
    "      print(f\"Copied '{filename}' to '{destination_folder}'.\")\n",
    "    except OSError as e:\n",
    "      print(f\"Error copying file '{filename}': {e}\")\n",
    "\n",
    "# Example usage\n",
    "source_txt_path = dataset_dir +\"/ImageSets/val.txt\"  # Replace with your actual path\n",
    "destination_folder_path = dataset_dir +\"/validation\"  # Replace with your actual path\n",
    "bin_folder_path = dataset_dir +\"/training/velodyne\"\n",
    "copy_bin_files(source_txt_path, bin_folder_path, destination_folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# inference\n",
    "# pcl = '../data/Dataset_Lidar_20240111_vegas_ontrack_01/validation'\n",
    "pcl = os.path.join(dataset_dir,\"validation\")\n",
    "pcl = os.path.join(dataset_dir,\"training\",\"velodyne\")\n",
    "# pcl = '../data/Hydra/training/velodyne/000020.bin'\n",
    "# pcl = '../data/DonaSet/training/velodyne/'\n",
    "# pcl = './data/falcon/falcon1.bin'\n",
    "inputs = dict(points=pcl)\n",
    "inf_res = inferencer(inputs, show=True,pred_score_thr=0.3, batch_size=1)\n",
    "inf_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "inferencer(inputs, show=False,pred_score_thr=0.3,batch_size=1)\n",
    "end = time.time()\n",
    "print(\"Time Taken {} ms\".format((end-start)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet3d.datasets.transforms.loading import (LoadAnnotations3D,\n",
    "                                                 LoadPointsFromFile)\n",
    "from mmdet3d.models.data_preprocessors.data_preprocessor import \\\n",
    "    Det3DDataPreprocessor\n",
    "from mmdet3d.datasets.transforms.formating import Pack3DDetInputs\n",
    "import os \n",
    "\n",
    "def prepare_input(inputs,batch_size=1):\n",
    "    model_inputs = {}\n",
    "    model_inputs[\"inputs\"] = {}\n",
    "    model_inputs[\"inputs\"][\"points\"] = []\n",
    "    max = batch_size\n",
    "\n",
    "    files = []\n",
    "    if not os.path.isdir(inputs[\"points\"]):\n",
    "        files.append(inputs[\"points\"])\n",
    "    else:\n",
    "        files = [inputs[\"points\"] + s for s in os.listdir(inputs[\"points\"])]\n",
    "    for i,sample_path in enumerate(files):\n",
    "        inp = {}\n",
    "        inp[\"lidar_points\"] = {}\n",
    "        inp[\"lidar_points\"][\"lidar_path\"] = sample_path\n",
    "\n",
    "\n",
    "        loader = LoadPointsFromFile(coord_type='LIDAR',load_dim=4,use_dim=4)\n",
    "        packer = Pack3DDetInputs(keys=['points'])\n",
    "\n",
    "        voxel_size = [0.16, 0.16, 4]\n",
    "        preprocessor = Det3DDataPreprocessor(\n",
    "            voxel=True,\n",
    "            voxel_layer=dict(\n",
    "                max_num_points=32,  # max_points_per_voxel\n",
    "                point_cloud_range=[0, -39.68, -3, 69.12, 39.68, 1],\n",
    "                voxel_size=voxel_size,\n",
    "                max_voxels=(16000, 40000)))\n",
    "\n",
    "        input = loader(inp)\n",
    "        input = packer(input)\n",
    "        # input[\"inputs\"][\"points\"] = input[\"inputs\"][\"points\"].cuda()\n",
    "        model_inputs[\"inputs\"][\"points\"].append(input[\"inputs\"][\"points\"])\n",
    "        if i == max-1:\n",
    "            print(\"Batch Size!\")\n",
    "            break\n",
    "    prep_input = preprocessor(model_inputs)\n",
    "    prep_input[\"inputs\"][\"voxels\"][\"voxels\"] = prep_input[\"inputs\"][\"voxels\"][\"voxels\"].cuda()\n",
    "    return prep_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_input = prepare_input({\"points\": pcl}, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = inferencer.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "s = time.time()\n",
    "\n",
    "prep_input = prepare_input({\"points\": pcl}, batch_size=1) # Preprocess\n",
    "res = model(prep_input[\"inputs\"],prep_input[\"data_samples\"], mode=\"tensor\") # Forward\n",
    "preds  = model.bbox_head.predict_by_feat(*res) # Postprocess\n",
    "\n",
    "print(\"Time: {} ms\".format((time.time()-s)*1000))\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_res = inferencer(inputs, show=False,pred_score_thr=0.3, batch_size=4)\n",
    "inf_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Manually the tensor to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet3d.models.dense_heads.anchor3d_head import Anchor3DHead\n",
    "\n",
    "# bbox_head=dict(\n",
    "#     type='Anchor3DHead',\n",
    "#     num_classes=3,\n",
    "#     in_channels=384,\n",
    "#     feat_channels=384,\n",
    "#     use_direction_classifier=True,\n",
    "#     assign_per_class=True,\n",
    "#     anchor_generator=dict(\n",
    "#         type='AlignedAnchor3DRangeGenerator',\n",
    "#         ranges=[\n",
    "#             [0, -39.68, -0.6, 69.12, 39.68, -0.6],\n",
    "#             [0, -39.68, -0.6, 69.12, 39.68, -0.6],\n",
    "#             [0, -39.68, -1.78, 69.12, 39.68, -1.78],\n",
    "#         ],\n",
    "#         sizes=[[0.8, 0.6, 1.73], [1.76, 0.6, 1.73], [3.9, 1.6, 1.56]],\n",
    "#         rotations=[0, 1.57],\n",
    "#         reshape_out=False),\n",
    "#     diff_rad_by_sin=True,\n",
    "#     bbox_coder=dict(type='DeltaXYZWLHRBBoxCoder')\n",
    "# )\n",
    "# model.bbox_head\n",
    "head = Anchor3DHead(\n",
    "    num_classes=1,\n",
    "    in_channels=384,\n",
    "    feat_channels=384,\n",
    "    use_direction_classifier=True,\n",
    "    assign_per_class=True,\n",
    "    anchor_generator=dict(\n",
    "                type='AlignedAnchor3DRangeGenerator',\n",
    "                ranges=[[0, -39.68, -1.78, 69.12, 39.68, -1.78]],\n",
    "                sizes=[[3.9, 1.6, 1.56]],\n",
    "                rotations=[0, 1.57],\n",
    "                reshape_out=True),\n",
    "    diff_rad_by_sin=True,\n",
    "    bbox_coder=dict(type='DeltaXYZWLHRBBoxCoder'),\n",
    "    test_cfg=model.test_cfg\n",
    "    )\n",
    "# preds  = head.predict_by_feat(*res)\n",
    "preds  = model.bbox_head.predict_by_feat(*res)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds[0][\"bboxes_3d\"],pred_res[0][\"bboxes_3d\"],inf_res[\"predictions\"][0][\"bboxes_3d\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = inferencer.model\n",
    "pcl_path = os.path.join(dataset_dir, \"training/velodyne/000020.bin\")\n",
    "model_path = \"deployed_models/hydra/end2end.onnx\"\n",
    "model_cfg = \"../configs/pointpillars/pointpillars_hydra_car.py\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.config import Config\n",
    "\n",
    "def prepare_input(inputs,batch_size=1, model_cfg = None, device=\"cuda\",data_samples=False, deterministic=True):\n",
    "    model_inputs = {}\n",
    "    model_inputs[\"inputs\"] = {}\n",
    "    model_inputs[\"inputs\"][\"points\"] = []\n",
    "    if data_samples:\n",
    "        model_inputs[\"data_samples\"] = []\n",
    "    max = batch_size\n",
    "\n",
    "    files = []\n",
    "    if not os.path.isdir(inputs[\"points\"]):\n",
    "        files.append(inputs[\"points\"])\n",
    "    else:\n",
    "        files = [inputs[\"points\"] + s for s in os.listdir(inputs[\"points\"])]\n",
    "    for i,sample_path in enumerate(files):\n",
    "        inp = {}\n",
    "        inp[\"lidar_points\"] = {}\n",
    "        inp[\"lidar_points\"][\"lidar_path\"] = sample_path\n",
    "\n",
    "\n",
    "        loader = LoadPointsFromFile(coord_type='LIDAR',load_dim=4,use_dim=4)\n",
    "        packer = Pack3DDetInputs(keys=['points'])\n",
    "\n",
    "        cfg = Config.fromfile(model_cfg)\n",
    "        cfg[\"model\"][\"data_preprocessor\"].pop(\"type\")\n",
    "        preprocessor = Det3DDataPreprocessor(**cfg[\"model\"][\"data_preprocessor\"])\n",
    "        if not deterministic:\n",
    "            preprocessor.cuda()\n",
    "        input = loader(inp)\n",
    "        input = packer(input)\n",
    "        # input[\"inputs\"][\"points\"] = input[\"inputs\"][\"points\"].cuda()\n",
    "        model_inputs[\"inputs\"][\"points\"].append(input[\"inputs\"][\"points\"])\n",
    "        if data_samples:\n",
    "            model_inputs[\"data_samples\"].append(input[\"data_samples\"])\n",
    "        if i == max-1:\n",
    "            # print(\"Batch Size!\")\n",
    "            break\n",
    "    prep_input = preprocessor(model_inputs)\n",
    "    if device == \"cuda\":\n",
    "        prep_input[\"inputs\"][\"voxels\"][\"voxels\"] = prep_input[\"inputs\"][\"voxels\"][\"voxels\"].cuda()\n",
    "    return prep_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference time comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = time.time()\n",
    "model.cpu()\n",
    "inputs = prepare_input({\"points\": pcl_path}, batch_size=1, model_cfg=model_cfg,device = \"cpu\")[\"inputs\"]\n",
    "\n",
    "s1 = time.time()\n",
    "val_output = model(inputs,mode=\"tensor\")\n",
    "s2 = time.time()\n",
    "preds  = model.bbox_head.predict_by_feat(*val_output)\n",
    "\n",
    "print(\"Pre: {} ms\".format((s1-s0)*1000))\n",
    "print(\"Inf: {} ms\".format((s2-s1)*1000))\n",
    "print(\"Post: {} ms\".format((time.time()-s2)*1000))\n",
    "print(\"Total: {} ms\".format((time.time()-s1)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(inf_res[\"predictions\"][0][\"bboxes_3d\"][0]) -  list(preds[0].bboxes_3d.cpu())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = time.time()\n",
    "model.cuda()\n",
    "inputs = prepare_input({\"points\": pcl_path}, batch_size=1, model_cfg =  model_cfg, deterministic=True)[\"inputs\"]\n",
    "\n",
    "s1 = time.time()\n",
    "val_output = model(inputs,mode=\"tensor\")\n",
    "s2 = time.time()\n",
    "preds  = model.bbox_head.predict_by_feat(*val_output)\n",
    "\n",
    "print(\"Pre: {} ms\".format((s1-s0)*1000))\n",
    "print(\"Inf: {} ms\".format((s2-s1)*1000))\n",
    "print(\"Post: {} ms\".format((time.time()-s2)*1000))\n",
    "print(\"Total: {} ms\".format((time.time()-s1)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(inf_res[\"predictions\"][0][\"bboxes_3d\"][0]) -  list(preds[0].bboxes_3d.cpu())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorRT Provider (ONNX Runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_sess = ort.InferenceSession(model_path,providers=['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0= time.time()\n",
    "inputs = prepare_input({\"points\": pcl_path}, batch_size=1, model_cfg = model_cfg, device=\"cuda\")[\"inputs\"]\n",
    "\n",
    "onnx_inputs = {'voxels': inputs[\"voxels\"][\"voxels\"].cpu().numpy().astype(np.float32), 'coors':inputs[\"voxels\"][\"coors\"].cpu().numpy(),'num_points':inputs[\"voxels\"][\"num_points\"].cpu().numpy()}\n",
    "\n",
    "s1 = time.time()\n",
    "outputs = ort_sess.run(None, onnx_inputs)\n",
    "s2 = time.time()\n",
    "outputs = [arr.astype(np.float32) for arr in outputs]\n",
    "preds  = model.bbox_head.predict_by_feat(*outputs)\n",
    "print(\"Pre: {} ms\".format((s1-s0)*1000))\n",
    "print(\"Inf: {} ms\".format((s2-s1)*1000))\n",
    "print(\"Post: {} ms\".format((time.time()-s2)*1000))\n",
    "print(\"Total: {} ms\".format((time.time()-s0)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(inf_res[\"predictions\"][0][\"bboxes_3d\"][0]) -  list(preds[0].bboxes_3d.cpu())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorRT Provider (ONNX Runtime) - FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnxconverter_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnxmltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx import load\n",
    "from onnxmltools.utils.float16_converter import convert_float_to_float16\n",
    "from onnx import save\n",
    "onnx_model = load(model_path)\n",
    "new_onnx_model = convert_float_to_float16(onnx_model)\n",
    "save(new_onnx_model,\"deployed_models/hydra/end2end_fp16.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_sess = ort.InferenceSession(\"deployed_models/hydra/end2end_fp16.onnx\",providers=['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0= time.time()\n",
    "inputs = prepare_input({\"points\": pcl_path}, batch_size=1, model_cfg = model_cfg,device=\"cuda\")[\"inputs\"]\n",
    "\n",
    "onnx_inputs = {'voxels': inputs[\"voxels\"][\"voxels\"].cpu().numpy().astype(np.float16), 'coors':inputs[\"voxels\"][\"coors\"].cpu().numpy(),'num_points':inputs[\"voxels\"][\"num_points\"].cpu().numpy()}\n",
    "\n",
    "s1 = time.time()\n",
    "outputs = ort_sess.run(None, onnx_inputs)\n",
    "s2 = time.time()\n",
    "outputs = [arr.astype(np.float32) for arr in outputs]\n",
    "preds  = model.bbox_head.predict_by_feat(*outputs)\n",
    "print(\"Pre: {} ms\".format((s1-s0)*1000))\n",
    "print(\"Inf: {} ms\".format((s2-s1)*1000))\n",
    "print(\"Post: {} ms\".format((time.time()-s2)*1000))\n",
    "print(\"Total: {} ms\".format((time.time()-s0)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(inf_res[\"predictions\"][0][\"bboxes_3d\"][0]) -  list(preds[0].bboxes_3d.cpu())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Provider  (ONNX Runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_sess = ort.InferenceSession(model_path,providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = prepare_input({\"points\": pcl_path}, batch_size=1)[\"inputs\"]\n",
    "onnx_inputs = {'voxels': inputs[\"voxels\"][\"voxels\"].cpu().numpy(), 'coors':inputs[\"voxels\"][\"coors\"].cpu().numpy(),'num_points':inputs[\"voxels\"][\"num_points\"].cpu().numpy()}\n",
    "\n",
    "s1 = time.time()\n",
    "outputs = ort_sess.run(None, onnx_inputs)\n",
    "s2 = time.time()\n",
    "preds  = model.bbox_head.predict_by_feat(*outputs)\n",
    "print(\"Inf: {} ms\".format((s2-s1)*1000))\n",
    "print(\"Post: {} ms\".format((time.time()-s2)*1000))\n",
    "print(\"Total: {} ms\".format((time.time()-s1)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU Provider  (ONNX Runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_sess = ort.InferenceSession(model_path,providers=[ 'CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = prepare_input({\"points\": pcl_path}, batch_size=1)[\"inputs\"]\n",
    "onnx_inputs = {'voxels': inputs[\"voxels\"][\"voxels\"].cpu().numpy(), 'coors':inputs[\"voxels\"][\"coors\"].cpu().numpy(),'num_points':inputs[\"voxels\"][\"num_points\"].cpu().numpy()}\n",
    "\n",
    "s1 = time.time()\n",
    "outputs = ort_sess.run(None, onnx_inputs)\n",
    "s2 = time.time()\n",
    "preds  = model.bbox_head.predict_by_feat(*outputs)\n",
    "print(\"Inf: {} ms\".format((s2-s1)*1000))\n",
    "print(\"Post: {} ms\".format((time.time()-s2)*1000))\n",
    "print(\"Total: {} ms\".format((time.time()-s1)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.config import Config\n",
    "\n",
    "def prepare_input(inputs,batch_size=1, model_cfg = None, device=\"cuda\",data_samples=False, deterministic=True):\n",
    "    model_inputs = {}\n",
    "    model_inputs[\"inputs\"] = {}\n",
    "    model_inputs[\"inputs\"][\"points\"] = []\n",
    "    if data_samples:\n",
    "        model_inputs[\"data_samples\"] = []\n",
    "    max = batch_size\n",
    "\n",
    "    files = []\n",
    "    if not os.path.isdir(inputs[\"points\"]):\n",
    "        files.append(inputs[\"points\"])\n",
    "    else:\n",
    "        files = [inputs[\"points\"] + s for s in os.listdir(inputs[\"points\"])]\n",
    "    for i,sample_path in enumerate(files):\n",
    "        inp = {}\n",
    "        inp[\"lidar_points\"] = {}\n",
    "        inp[\"lidar_points\"][\"lidar_path\"] = sample_path\n",
    "\n",
    "\n",
    "        loader = LoadPointsFromFile(coord_type='LIDAR',load_dim=4,use_dim=4)\n",
    "        packer = Pack3DDetInputs(keys=['points'])\n",
    "\n",
    "        cfg = Config.fromfile(model_cfg)\n",
    "        cfg[\"model\"][\"data_preprocessor\"].pop(\"type\")\n",
    "        preprocessor = Det3DDataPreprocessor(**cfg[\"model\"][\"data_preprocessor\"])\n",
    "        if not deterministic:\n",
    "            preprocessor.cuda()\n",
    "        input = loader(inp)\n",
    "        input = packer(input)\n",
    "        # input[\"inputs\"][\"points\"] = input[\"inputs\"][\"points\"].cuda()\n",
    "        model_inputs[\"inputs\"][\"points\"].append(input[\"inputs\"][\"points\"])\n",
    "        if data_samples:\n",
    "            model_inputs[\"data_samples\"].append(input[\"data_samples\"])\n",
    "        if i == max-1:\n",
    "            # print(\"Batch Size!\")\n",
    "            break\n",
    "    prep_input = preprocessor(model_inputs)\n",
    "    if device == \"cuda\":\n",
    "        prep_input[\"inputs\"][\"voxels\"][\"voxels\"] = prep_input[\"inputs\"][\"voxels\"][\"voxels\"].cuda()\n",
    "    return prep_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model_cfg = \"../configs/pointpillars/pointpillars_hydris_car.py\"\n",
    "deploy_cfg = \"../../mmdeploy/configs/mmdet3d/voxel-detection/voxel-detection_tensorrt_dynamic-kitti-32x4_fp16.py\"\n",
    "backend_files = \"deployed_models/hydra/end2end.engine\"\n",
    "img = os.path.join(dataset_dir,\"training/velodyne/000000.bin\")\n",
    "device = \"cuda\"\n",
    "\n",
    "tensorrt_model = get_model(model_cfg,deploy_cfg,[backend_files],img,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcl_path =  os.path.join(dataset_dir,\"training/velodyne/000020.bin\")\n",
    "s0 = time.time()\n",
    "inputs = prepare_input({\"points\": pcl_path}, batch_size=1,model_cfg = model_cfg, data_samples=False)[\"inputs\"]\n",
    "\n",
    "s1=time.time()\n",
    "res = tensorrt_model(inputs, mode=\"tensor\")\n",
    "s2 =time.time()\n",
    "preds  = model.bbox_head.predict_by_feat(res[\"cls_score\"],res[\"bbox_pred\"],res[\"dir_cls_pred\"])\n",
    "print(\"Pre: {} ms\".format((s1-s0)*1000))\n",
    "print(\"Inf: {} ms\".format((s2-s1)*1000))\n",
    "print(\"Post: {} ms\".format((time.time()-s2)*1000))\n",
    "print(\"Total: {} ms\".format((time.time()-s0)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(inf_res[\"predictions\"][0][\"bboxes_3d\"][0]) -  list(preds[0].bboxes_3d.cpu())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Inferencer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.config import Config\n",
    "from mmdet3d.models.dense_heads import Anchor3DHead\n",
    "\n",
    "cfg = Config.fromfile(model_cfg)\n",
    "\n",
    "cfg[\"model\"][\"bbox_head\"].pop(\"type\")\n",
    "bbox_head = Anchor3DHead(**cfg[\"model\"][\"bbox_head\"])\n",
    "bbox_head.test_cfg = cfg[\"model\"][\"test_cfg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = time.time()\n",
    "inputs = prepare_input({\"points\": pcl_path}, batch_size=1, model_cfg=model_cfg,data_samples=False)[\"inputs\"]\n",
    "\n",
    "s1=time.time()\n",
    "res = tensorrt_model(inputs, mode=\"tensor\")\n",
    "s2 =time.time()\n",
    "preds  = bbox_head.predict_by_feat(res[\"cls_score\"],res[\"bbox_pred\"],res[\"dir_cls_pred\"])\n",
    "print(\"Pre: {} ms\".format((s1-s0)*1000))\n",
    "print(\"Inf: {} ms\".format((s2-s1)*1000))\n",
    "print(\"Post: {} ms\".format((time.time()-s2)*1000))\n",
    "print(\"Total: {} ms\".format((time.time()-s0)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(inf_res[\"predictions\"][0][\"bboxes_3d\"][0]) -  list(preds[0].bboxes_3d.cpu())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorRT Fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# model_cfg = \"../configs/pointpillars/pointpillars_hydris_car.py\"\n",
    "deploy_cfg = \"../../mmdeploy/configs/mmdet3d/voxel-detection/voxel-detection_tensorrt_dynamic-kitti-32x4_fp16.py\"\n",
    "backend_files = \"deployed_models/hydra/end2end_fp16.engine\"\n",
    "img =  os.path.join(dataset_dir,\"velodyne/000000.bin\")\n",
    "device = \"cuda\"\n",
    "\n",
    "tensorrt_model = get_model(model_cfg,deploy_cfg,[backend_files],img,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = time.time()\n",
    "inputs = prepare_input({\"points\": pcl_path}, batch_size=1,model_cfg=model_cfg,data_samples=False)[\"inputs\"]\n",
    "\n",
    "s1=time.time()\n",
    "res = tensorrt_model(inputs, mode=\"tensor\")\n",
    "s2 =time.time()\n",
    "preds  = model.bbox_head.predict_by_feat(res[\"cls_score\"],res[\"bbox_pred\"],res[\"dir_cls_pred\"])\n",
    "print(\"Pre: {} ms\".format((s1-s0)*1000))\n",
    "print(\"Inf: {} ms\".format((s2-s1)*1000))\n",
    "print(\"Post: {} ms\".format((time.time()-s2)*1000))\n",
    "print(\"Total: {} ms\".format((time.time()-s0)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(inf_res[\"predictions\"][0][\"bboxes_3d\"][0]) -  list(preds[0].bboxes_3d.cpu())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = dict(points=pcl)\n",
    "inf_res = inferencer(inputs, show=True, pred_score_thr=0.3, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet3d.structures import Det3DDataSample\n",
    "\n",
    "inputs = prepare_input({\"points\": pcl}, batch_size=1,model_cfg=model_cfg,data_samples=False)[\"inputs\"]\n",
    "preds = tensorrt_model(inputs, mode=\"predict\")\n",
    "ds = Det3DDataSample()\n",
    "ds.pred_instances_3d = preds[0]\n",
    "\n",
    "inferencer.visualize([{\"points\": pcl}], [ds],show=True,pred_score_thr=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(inf_res[\"predictions\"][0][\"bboxes_3d\"][0]) -  list(ds.pred_instances_3d.bboxes_3d.cpu())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voxelization algorithm comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet3d.models.data_preprocessors import Det3DDataPreprocessor\n",
    "\n",
    "\n",
    "def prepare_input(inputs,batch_size=1, device=\"cuda\",data_samples=False, deterministic=True):\n",
    "    model_inputs = {}\n",
    "    model_inputs[\"inputs\"] = {}\n",
    "    model_inputs[\"inputs\"][\"points\"] = []\n",
    "    if data_samples:\n",
    "        model_inputs[\"data_samples\"] = []\n",
    "    max = batch_size\n",
    "\n",
    "    files = []\n",
    "    if not os.path.isdir(inputs[\"points\"]):\n",
    "        files.append(inputs[\"points\"])\n",
    "    else:\n",
    "        files = [inputs[\"points\"] + s for s in os.listdir(inputs[\"points\"])]\n",
    "    for i,sample_path in enumerate(files):\n",
    "        inp = {}\n",
    "        inp[\"lidar_points\"] = {}\n",
    "        inp[\"lidar_points\"][\"lidar_path\"] = sample_path\n",
    "\n",
    "\n",
    "        loader = LoadPointsFromFile(coord_type='LIDAR',load_dim=4,use_dim=4)\n",
    "        packer = Pack3DDetInputs(keys=['points'])\n",
    "\n",
    "        voxel_size = [0.16, 0.16, 4]\n",
    "        preprocessor = Det3DDataPreprocessor(\n",
    "            voxel=True,\n",
    "            voxel_layer=dict(\n",
    "                max_num_points=32,  # max_points_per_voxel\n",
    "                point_cloud_range=[-60, -19.84, -3, 79.52, 19.84, 3],\n",
    "                # point_cloud_range=[0, -39.68, -3, 69.12, 39.68, 3],\n",
    "                voxel_size=voxel_size,\n",
    "                max_voxels=(16000, 40000),\n",
    "                deterministic=deterministic))\n",
    "        if not deterministic:\n",
    "            preprocessor.cuda()\n",
    "        input = loader(inp)\n",
    "        input = packer(input)\n",
    "        # input[\"inputs\"][\"points\"] = input[\"inputs\"][\"points\"].cuda()\n",
    "        model_inputs[\"inputs\"][\"points\"].append(input[\"inputs\"][\"points\"])\n",
    "        if data_samples:\n",
    "            model_inputs[\"data_samples\"].append(input[\"data_samples\"])\n",
    "        if i == max-1:\n",
    "            # print(\"Batch Size!\")\n",
    "            break\n",
    "    prep_input = preprocessor(model_inputs)\n",
    "    if device == \"cuda\":\n",
    "        prep_input[\"inputs\"][\"voxels\"][\"voxels\"] = prep_input[\"inputs\"][\"voxels\"][\"voxels\"].cuda()\n",
    "    return prep_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = time.time()\n",
    "inputs = prepare_input({\"points\": pcl_path}, batch_size=1,data_samples=False, deterministic=False, device=\"cuda\")[\"inputs\"]\n",
    "print(\"Non deterministic voxel -> \" +str((time.time()-s0)*1000))\n",
    "\n",
    "s0 = time.time()\n",
    "inputs = prepare_input({\"points\": pcl_path}, batch_size=1,data_samples=False, deterministic=True, device=\"cuda\")[\"inputs\"]\n",
    "print(\"Deterministic voxel -> \" +str((time.time()-s0)*1000))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a0c343fece975dd89087e8c2194dd4d3db28d7000f1b32ed9ed9d584dd54dbbe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
