{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/root/workspace/LidarObjDetection/mmdetection3d_dona/mmdetection3d\")\n",
    "sys.path.append(\"/root/workspace/LidarObjDetection/mmdeploy_dona/mmdeploy\")\n",
    "\n",
    "from mmdet3d.apis import LidarDet3DInferencer\n",
    "import os\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import open3d as o3d\n",
    "from mmdet3d.datasets.transforms.loading import (LoadAnnotations3D,\n",
    "                                                LoadPointsFromFile)\n",
    "from mmdet3d.models.data_preprocessors.data_preprocessor import \\\n",
    "    Det3DDataPreprocessor\n",
    "import onnxruntime as ort\n",
    "from mmdet3d.datasets.transforms.formating import Pack3DDetInputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: /root/workspace/LidarObjDetection/mmdetection3d_dona/mmdetection3d/work_dirs/pointpillars_hv_secfpn_8xb6-160e_cielo_falcon-3d-car/iter_5640.pth\n",
      "02/28 12:58:16 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Failed to search registry with scope \"mmdet3d\" in the \"function\" registry tree. As a workaround, the current \"function\" registry in \"mmengine\" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether \"mmdet3d\" is a correct scope, or whether the registry is initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/workspace/LidarObjDetection/mmdetection3d_dona/mmdetection3d/mmdet3d/models/dense_heads/anchor3d_head.py:94: UserWarning: dir_offset and dir_limit_offset will be depressed and be incorporated into box coder in the future\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/mmengine/visualization/visualizer.py:196: UserWarning: Failed to add <class 'mmengine.visualization.vis_backend.LocalVisBackend'>, please provide the `save_dir` argument.\n",
      "  warnings.warn(f'Failed to add {vis_backend.__class__}, '\n"
     ]
    }
   ],
   "source": [
    "# initialize inferencer\n",
    "# CHANGE THE center_mode in loca_visualizer accordingly (KITTI -> center_mode = 'lidar_bottom', CUSTOM -> center_mode = whatever)\n",
    "\n",
    "# inferencer = LidarDet3DInferencer('pointpillars_hv_secfpn_8xb6-160e_kitti-3d-car')\n",
    "# inferencer = LidarDet3DInferencer('pointpillars_donaset_container-car')\n",
    "inferencer = LidarDet3DInferencer('pointpillars_cielo_falcon-car')\n",
    "\n",
    "\n",
    "# inferencer = LidarDet3DInferencer('pointpillars_kittilidar-car') \n",
    "# inferencer = LidarDet3DInferencer(\"pointpillars_hv_secfpn_8xb6-160e_kittilidar-3d-car\",scope=\"mmdet3d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33m[Open3D WARNING] [ViewControl] ConvertFromPinholeCameraParameters() failed because window height and width do not match.\u001b[0;m\n",
      "\u001b[1;33m[Open3D WARNING] GLFW Error: The GLFW library is not initialized\u001b[0;m\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "pcl = '../data/cielo_falcon/training/velodyne/'\n",
    "# pcl = './data/falcon/falcon1.bin'\n",
    "\n",
    "inputs = dict(points=pcl)\n",
    "inf_res = inferencer(inputs, show=True,pred_score_thr=0.3, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing took 40.65680503845215 ms\n",
      "Voxel Encoder took 0.7505416870117188 ms\n",
      "Middle Encoder/Imagify took 54.91042137145996 ms\n",
      "Backbone + FPN took 1.1451244354248047 ms\n",
      "Bbox Head pred took 152.24933624267578 ms\n",
      "\n",
      "Time Taken 262.5012397766113 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "inferencer(inputs, show=False,pred_score_thr=0.3,batch_size=4)\n",
    "end = time.time()\n",
    "print(\"Time Taken {} ms\".format((end-start)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet3d.datasets.transforms.loading import (LoadAnnotations3D,\n",
    "                                                 LoadPointsFromFile)\n",
    "from mmdet3d.models.data_preprocessors.data_preprocessor import \\\n",
    "    Det3DDataPreprocessor\n",
    "from mmdet3d.datasets.transforms.formating import Pack3DDetInputs\n",
    "import os \n",
    "\n",
    "def prepare_input(inputs,batch_size=1):\n",
    "    model_inputs = {}\n",
    "    model_inputs[\"inputs\"] = {}\n",
    "    model_inputs[\"inputs\"][\"points\"] = []\n",
    "    max = batch_size\n",
    "\n",
    "    files = []\n",
    "    if not os.path.isdir(inputs[\"points\"]):\n",
    "        files.append(inputs[\"points\"])\n",
    "    else:\n",
    "        files = [inputs[\"points\"] + s for s in os.listdir(inputs[\"points\"])]\n",
    "    for i,sample_path in enumerate(files):\n",
    "        inp = {}\n",
    "        inp[\"lidar_points\"] = {}\n",
    "        inp[\"lidar_points\"][\"lidar_path\"] = sample_path\n",
    "\n",
    "\n",
    "        loader = LoadPointsFromFile(coord_type='LIDAR',load_dim=4,use_dim=4)\n",
    "        packer = Pack3DDetInputs(keys=['points'])\n",
    "\n",
    "        voxel_size = [0.16, 0.16, 4]\n",
    "        preprocessor = Det3DDataPreprocessor(\n",
    "            voxel=True,\n",
    "            voxel_layer=dict(\n",
    "                max_num_points=32,  # max_points_per_voxel\n",
    "                point_cloud_range=[0, -39.68, -3, 69.12, 39.68, 1],\n",
    "                voxel_size=voxel_size,\n",
    "                max_voxels=(16000, 40000)))\n",
    "\n",
    "        input = loader(inp)\n",
    "        input = packer(input)\n",
    "        # input[\"inputs\"][\"points\"] = input[\"inputs\"][\"points\"].cuda()\n",
    "        model_inputs[\"inputs\"][\"points\"].append(input[\"inputs\"][\"points\"])\n",
    "        if i == max-1:\n",
    "            print(\"Batch Size!\")\n",
    "            break\n",
    "    prep_input = preprocessor(model_inputs)\n",
    "    prep_input[\"inputs\"][\"voxels\"][\"voxels\"] = prep_input[\"inputs\"][\"voxels\"][\"voxels\"].cuda()\n",
    "    return prep_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size!\n",
      "Preprocessing took 4.820346832275391 ms\n"
     ]
    }
   ],
   "source": [
    "prep_input = prepare_input({\"points\": pcl}, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = inferencer.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[68, 2, 3]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "b = a\n",
    "b[0] = 68\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size!\n",
      "Preprocessing took 4.385232925415039 ms\n",
      "Voxel Encoder took 3.142118453979492 ms\n",
      "Middle Encoder/Imagify took 8.364200592041016 ms\n",
      "Backbone + FPN took 3.4232139587402344 ms\n",
      "Bbox Head forward took 0.274658203125 ms\n",
      "\n",
      "Time: 66.46871566772461 ms\n",
      "[<InstanceData(\n",
      "\n",
      "    META INFORMATION\n",
      "\n",
      "    DATA FIELDS\n",
      "    bboxes_3d: LiDARInstance3DBoxes(\n",
      "            tensor([[3.8503e+01, 2.7783e-01, 4.6321e-01, 4.8079e+00, 1.8755e+00, 1.2150e+00,\n",
      "                 2.7990e-02]], device='cuda:0'))\n",
      "    labels_3d: tensor([0], device='cuda:0')\n",
      "    scores_3d: tensor([0.9256], device='cuda:0')\n",
      ") at 0x7f8f8da84d60>]\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "s = time.time()\n",
    "\n",
    "prep_input = prepare_input({\"points\": pcl}, batch_size=1) # Preprocess\n",
    "res = model(prep_input[\"inputs\"],prep_input[\"data_samples\"], mode=\"tensor\") # Forward\n",
    "preds  = model.bbox_head.predict_by_feat(*res) # Postprocess\n",
    "\n",
    "print(\"Time: {} ms\".format((time.time()-s)*1000))\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing took 10.436296463012695 ms\n",
      "Voxel Encoder took 27009.531021118164 ms\n",
      "Middle Encoder/Imagify took 5892.206907272339 ms\n",
      "Backbone + FPN took 9514.796257019043 ms\n",
      "Bbox Head pred took 3506.218433380127 ms\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'predictions': [{'labels_3d': [0],\n",
       "   'scores_3d': [0.9243199229240417],\n",
       "   'bboxes_3d': [[38.9719123840332,\n",
       "     0.44309741258621216,\n",
       "     0.4683852791786194,\n",
       "     4.806761741638184,\n",
       "     1.8732784986495972,\n",
       "     1.2071927785873413,\n",
       "     0.15700256824493408]],\n",
       "   'box_type_3d': 'LiDAR'}],\n",
       " 'visualization': []}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_res = inferencer(inputs, show=False,pred_score_thr=0.3, batch_size=4)\n",
    "inf_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Manually the tensor to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/riccardo/LidarObjDetection/mmdetection3d_dona/mmdetection3d/mmdet3d/models/dense_heads/anchor3d_head.py:94: UserWarning: dir_offset and dir_limit_offset will be depressed and be incorporated into box coder in the future\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<InstanceData(\n",
       " \n",
       "     META INFORMATION\n",
       " \n",
       "     DATA FIELDS\n",
       "     bboxes_3d: LiDARInstance3DBoxes(\n",
       "             tensor([[3.8503e+01, 2.7785e-01, 4.6314e-01, 4.8079e+00, 1.8755e+00, 1.2150e+00,\n",
       "                  2.7934e-02]], device='cuda:0'))\n",
       "     scores_3d: tensor([0.9255], device='cuda:0')\n",
       "     labels_3d: tensor([0], device='cuda:0')\n",
       " ) at 0x7f3a9b63fa90>,\n",
       " <InstanceData(\n",
       " \n",
       "     META INFORMATION\n",
       " \n",
       "     DATA FIELDS\n",
       "     bboxes_3d: LiDARInstance3DBoxes(\n",
       "             tensor([[39.3502,  0.1765,  0.4314,  4.7754,  1.8597,  1.1957,  0.1487]],\n",
       "                device='cuda:0'))\n",
       "     scores_3d: tensor([0.9433], device='cuda:0')\n",
       "     labels_3d: tensor([0], device='cuda:0')\n",
       " ) at 0x7f3a9b63f850>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mmdet3d.models.dense_heads.anchor3d_head import Anchor3DHead\n",
    "\n",
    "# bbox_head=dict(\n",
    "#     type='Anchor3DHead',\n",
    "#     num_classes=3,\n",
    "#     in_channels=384,\n",
    "#     feat_channels=384,\n",
    "#     use_direction_classifier=True,\n",
    "#     assign_per_class=True,\n",
    "#     anchor_generator=dict(\n",
    "#         type='AlignedAnchor3DRangeGenerator',\n",
    "#         ranges=[\n",
    "#             [0, -39.68, -0.6, 69.12, 39.68, -0.6],\n",
    "#             [0, -39.68, -0.6, 69.12, 39.68, -0.6],\n",
    "#             [0, -39.68, -1.78, 69.12, 39.68, -1.78],\n",
    "#         ],\n",
    "#         sizes=[[0.8, 0.6, 1.73], [1.76, 0.6, 1.73], [3.9, 1.6, 1.56]],\n",
    "#         rotations=[0, 1.57],\n",
    "#         reshape_out=False),\n",
    "#     diff_rad_by_sin=True,\n",
    "#     bbox_coder=dict(type='DeltaXYZWLHRBBoxCoder')\n",
    "# )\n",
    "# model.bbox_head\n",
    "head = Anchor3DHead(\n",
    "    num_classes=1,\n",
    "    in_channels=384,\n",
    "    feat_channels=384,\n",
    "    use_direction_classifier=True,\n",
    "    assign_per_class=True,\n",
    "    anchor_generator=dict(\n",
    "                type='AlignedAnchor3DRangeGenerator',\n",
    "                ranges=[[0, -39.68, -1.78, 69.12, 39.68, -1.78]],\n",
    "                sizes=[[3.9, 1.6, 1.56]],\n",
    "                rotations=[0, 1.57],\n",
    "                reshape_out=True),\n",
    "    diff_rad_by_sin=True,\n",
    "    bbox_coder=dict(type='DeltaXYZWLHRBBoxCoder'),\n",
    "    test_cfg=model.test_cfg\n",
    "    )\n",
    "# preds  = head.predict_by_feat(*res)\n",
    "preds  = model.bbox_head.predict_by_feat(*res)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LiDARInstance3DBoxes(\n",
       "     tensor([[1.3809, 0.3621, 0.8436, 5.5223, 2.1287, 1.3461, 0.2666]],\n",
       "        device='cuda:0')),\n",
       " LiDARInstance3DBoxes(\n",
       "     tensor([[38.9716,  0.4431,  0.4685,  4.8068,  1.8733,  1.2072,  0.1570]],\n",
       "        device='cuda:0')),\n",
       " [[38.97159194946289,\n",
       "   0.44305962324142456,\n",
       "   0.4684826135635376,\n",
       "   4.80681848526001,\n",
       "   1.8733068704605103,\n",
       "   1.2071855068206787,\n",
       "   0.15700531005859375]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0][\"bboxes_3d\"],pred_res[0][\"bboxes_3d\"],inf_res[\"predictions\"][0][\"bboxes_3d\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = inferencer.model\n",
    "pcl_path = \"../data/cielo_falcon/training/velodyne/\"\n",
    "model_path = \"end2end.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_preds(preds,inputs,thr=0.3):\n",
    "    def create_oriented_bounding_box(x, y, z, dx, dy, dz, yaw):\n",
    "        box = o3d.geometry.OrientedBoundingBox()\n",
    "        box.center = [x, y, z]\n",
    "        box.extent = [dx , dy, dz ]\n",
    "        box.R = o3d.geometry.get_rotation_matrix_from_xyz([0, 0, yaw])\n",
    "        box.color=[1,0,0]\n",
    "        return box\n",
    "    for i in range(len(preds)):\n",
    "        # Create an Open3D point cloud object\n",
    "        point_cloud = o3d.geometry.PointCloud()\n",
    "        point_cloud.points = o3d.utility.Vector3dVector(inputs[\"points\"][i][:,:3])\n",
    "        point_cloud.colors = o3d.utility.Vector3dVector(np.zeros(inputs[\"points\"][i][:,:3].shape))  # Set all point colors to black\n",
    "        # x, y, z, dx, dy, dz, yaw = preds[i].bboxes_3d.cpu().numpy().flatten()\n",
    "        p = preds[i].bboxes_3d.cpu().numpy()\n",
    "        sc = preds[i].scores_3d.cpu().numpy()\n",
    "        geometries_to_draw = [point_cloud]\n",
    "        for j in range(p.shape[0]):\n",
    "            if sc[i] > thr:\n",
    "                x, y, z, dx, dy, dz, yaw =p[j,:].flatten()\n",
    "                box = create_oriented_bounding_box(x, y, z, dx, dy, dz, yaw)\n",
    "                geometries_to_draw.append(box)\n",
    "        o3d.visualization.draw_geometries(geometries_to_draw)\n",
    "\n",
    "def prepare_input(inputs,batch_size=1, device=\"cuda\",data_samples=False):\n",
    "    model_inputs = {}\n",
    "    model_inputs[\"inputs\"] = {}\n",
    "    model_inputs[\"inputs\"][\"points\"] = []\n",
    "    if data_samples:\n",
    "        model_inputs[\"data_samples\"] = []\n",
    "    max = batch_size\n",
    "\n",
    "    files = []\n",
    "    if not os.path.isdir(inputs[\"points\"]):\n",
    "        files.append(inputs[\"points\"])\n",
    "    else:\n",
    "        files = [inputs[\"points\"] + s for s in os.listdir(inputs[\"points\"])]\n",
    "    for i,sample_path in enumerate(files):\n",
    "        inp = {}\n",
    "        inp[\"lidar_points\"] = {}\n",
    "        inp[\"lidar_points\"][\"lidar_path\"] = sample_path\n",
    "\n",
    "\n",
    "        loader = LoadPointsFromFile(coord_type='LIDAR',load_dim=4,use_dim=4)\n",
    "        packer = Pack3DDetInputs(keys=['points'])\n",
    "\n",
    "        voxel_size = [0.16, 0.16, 4]\n",
    "        preprocessor = Det3DDataPreprocessor(\n",
    "            voxel=True,\n",
    "            voxel_layer=dict(\n",
    "                max_num_points=32,  # max_points_per_voxel\n",
    "                point_cloud_range=[0, -39.68, -3, 69.12, 39.68, 1],\n",
    "                voxel_size=voxel_size,\n",
    "                max_voxels=(16000, 40000)))\n",
    "\n",
    "        input = loader(inp)\n",
    "        input = packer(input)\n",
    "        # input[\"inputs\"][\"points\"] = input[\"inputs\"][\"points\"].cuda()\n",
    "        model_inputs[\"inputs\"][\"points\"].append(input[\"inputs\"][\"points\"])\n",
    "        if data_samples:\n",
    "            model_inputs[\"data_samples\"].append(input[\"data_samples\"])\n",
    "        if i == max-1:\n",
    "            # print(\"Batch Size!\")\n",
    "            break\n",
    "    prep_input = preprocessor(model_inputs)\n",
    "    if device == \"cuda\":\n",
    "        prep_input[\"inputs\"][\"voxels\"][\"voxels\"] = prep_input[\"inputs\"][\"voxels\"][\"voxels\"].cuda()\n",
    "    return prep_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # print(\"Batch Size!\")\n",
    "            break\n",
    "    prep_input = preprocessor(model_inputs)\n",
    "    if device == \"cuda\":\n",
    "        prep_input[\"inputs\"][\"voxels\"][\"voxels\"] = prep_input[\"inputs\"][\"voxels\"][\"voxels\"].cuda()\n",
    "    return prep_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000224\n",
      "\n",
      "000148\n",
      "\n",
      "000120\n",
      "\n",
      "000181\n",
      "\n",
      "000188\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m inputs \u001b[38;5;241m=\u001b[39m prepare_input({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(pcl_path,line[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m)}, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m preds \u001b[38;5;241m=\u001b[39m model(inputs,mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mvisualize_preds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mthr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m, in \u001b[0;36mvisualize_preds\u001b[0;34m(preds, inputs, thr)\u001b[0m\n\u001b[1;32m     21\u001b[0m         box \u001b[38;5;241m=\u001b[39m create_oriented_bounding_box(x, y, z, dx, dy, dz, yaw)\n\u001b[1;32m     22\u001b[0m         geometries_to_draw\u001b[38;5;241m.\u001b[39mappend(box)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mo3d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_geometries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeometries_to_draw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pcl_path = \"../data/cielo_falcon/training/velodyne/\"\n",
    "val_path = \"../data/cielo_falcon/ImageSets/val.txt\"\n",
    "\n",
    "with open(val_path, 'r') as file:\n",
    "    for line in file:\n",
    "        print(line)\n",
    "        inputs = prepare_input({\"points\": os.path.join(pcl_path,line[:-1]+\".bin\")}, batch_size=1)[\"inputs\"]\n",
    "        preds = model(inputs,mode=\"predict\")\n",
    "        visualize_preds(preds,inputs,thr=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size!\n",
      "Time: 26.506423950195312 ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import time\n",
    "from mmdet3d.datasets.transforms.loading import (LoadAnnotations3D,\n",
    "                                                LoadPointsFromFile)\n",
    "from mmdet3d.models.data_preprocessors.data_preprocessor import \\\n",
    "    Det3DDataPreprocessor\n",
    "from mmdet3d.datasets.transforms.formating import Pack3DDetInputs\n",
    "\n",
    "pcl_path = \"../data/DonaSet/training/velodyne/\"\n",
    "s = time.time()\n",
    "\n",
    "inputs = prepare_input({\"points\": pcl_path}, batch_size=1)[\"inputs\"]\n",
    "# Evaluate on validation data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_output = model(inputs,mode=\"tensor\")\n",
    "    print(\"Time: {} ms\".format((time.time()-s)*1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model Operator Set Version: 11\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model_path = \"end2end.onnx\"\n",
    "m = onnx.load(onnx_model_path)\n",
    "opset_version = m.opset_import[0].version if m.opset_import else None\n",
    "\n",
    "print(\"ONNX Model Operator Set Version:\", opset_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Runtime Version: 1.15.1\n",
      "GPU\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_version = onnxruntime.__version__\n",
    "print(\"ONNX Runtime Version:\", ort_version)\n",
    "\n",
    "print( onnxruntime.get_device()  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 12:29:06.650643080 [W:onnxruntime:Default, tensorrt_execution_provider.h:75 log] [2024-02-22 12:29:06 WARNING] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "2024-02-22 12:29:06.650668472 [W:onnxruntime:Default, tensorrt_execution_provider.h:75 log] [2024-02-22 12:29:06 WARNING] onnx2trt_utils.cpp:400: One or more weights outside the range of INT32 was clamped\n",
      "2024-02-22 12:29:06.729307543 [W:onnxruntime:Default, tensorrt_execution_provider.h:75 log] [2024-02-22 12:29:06 WARNING] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "2024-02-22 12:29:06.729332463 [W:onnxruntime:Default, tensorrt_execution_provider.h:75 log] [2024-02-22 12:29:06 WARNING] onnx2trt_utils.cpp:400: One or more weights outside the range of INT32 was clamped\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "model_path = 'end2end.onnx'\n",
    "onnx_model = onnx.load(model_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "ort_sess = ort.InferenceSession(model_path,providers=['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size!\n",
      "Preprocessing took 2.275705337524414 ms\n",
      "Preprocessing took: 4.957437515258789 ms\n",
      "Cpu conv took: 1.6481876373291016 ms\n",
      "Inference took: 18684.181928634644 ms\n",
      "Postprocessing took: 3.123760223388672 ms\n",
      "Total: 18693.91131401062 ms\n"
     ]
    }
   ],
   "source": [
    "s0 = time.time()\n",
    "inputs = prepare_input({\"points\": pcl_path}, batch_size=1)[\"inputs\"]\n",
    "s05 = time.time()\n",
    "onnx_inputs = {'voxels': inputs[\"voxels\"][\"voxels\"].cpu().numpy(), 'coors':inputs[\"voxels\"][\"coors\"].cpu().numpy(),'num_points':inputs[\"voxels\"][\"num_points\"].cpu().numpy()}\n",
    "s1 = time.time()\n",
    "outputs = ort_sess.run(None, onnx_inputs)\n",
    "s2 = time.time()\n",
    "preds  = model.bbox_head.predict_by_feat(*outputs)\n",
    "s3 = time.time()\n",
    "\n",
    "print(\"Preprocessing took: {} ms\".format((s05-s0)*1000))\n",
    "print(\"Cpu conv took: {} ms\".format((s1-s05)*1000))\n",
    "print(\"Inference took: {} ms\".format((s2-s1)*1000))\n",
    "print(\"Postprocessing took: {} ms\".format((s3-s2)*1000))\n",
    "print(\"Total: {} ms\".format((s3-s0)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size!\n",
      "Preprocessing took 2.525806427001953 ms\n"
     ]
    }
   ],
   "source": [
    "inputs = prepare_input({\"points\": pcl_path}, batch_size=1)[\"inputs\"]\n",
    "s05 = time.time()\n",
    "onnx_inputs = {'voxels': inputs[\"voxels\"][\"voxels\"].cpu().numpy(), 'coors':inputs[\"voxels\"][\"coors\"].cpu().numpy(),'num_points':inputs[\"voxels\"][\"num_points\"].cpu().numpy()}\n",
    "s1 = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference time comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size!\n",
      "Inf: 289.40629959106445 ms\n",
      "Post: 5.438566207885742 ms\n",
      "Total: 294.905424118042 ms\n"
     ]
    }
   ],
   "source": [
    "model.cpu()\n",
    "inputs = prepare_input({\"points\": pcl_path}, batch_size=1, device = \"cpu\")[\"inputs\"]\n",
    "\n",
    "s1 = time.time()\n",
    "val_output = model(inputs,mode=\"tensor\")\n",
    "s2 = time.time()\n",
    "preds  = model.bbox_head.predict_by_feat(*val_output)\n",
    "\n",
    "print(\"Inf: {} ms\".format((s2-s1)*1000))\n",
    "print(\"Post: {} ms\".format((time.time()-s2)*1000))\n",
    "print(\"Total: {} ms\".format((time.time()-s1)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inf: 13.108491897583008 ms\n",
      "Post: 35.6135368347168 ms\n",
      "Total: 48.75373840332031 ms\n"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "inputs = prepare_input({\"points\": pcl_path}, batch_size=1)[\"inputs\"]\n",
    "\n",
    "s1 = time.time()\n",
    "val_output = model(inputs,mode=\"tensor\")\n",
    "s2 = time.time()\n",
    "preds  = model.bbox_head.predict_by_feat(*val_output)\n",
    "print(\"Inf: {} ms\".format((s2-s1)*1000))\n",
    "print(\"Post: {} ms\".format((time.time()-s2)*1000))\n",
    "print(\"Total: {} ms\".format((time.time()-s1)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LiDARInstance3DBoxes(\n",
       "    tensor([[ 32.2025,  -4.3633,   0.6378,   4.0179,   1.6634,   1.6580,  -0.1687],\n",
       "        [ 44.4085,   4.6570,   0.6888,   3.9835,   1.6474,   1.6372,   0.1977],\n",
       "        [ 40.4617,   1.3992,   0.6364,   3.9788,   1.6458,   1.6358,  -0.0796],\n",
       "        [ 12.4731,  10.0320,   0.6872,   4.4271,   1.8337,   1.8257,   1.1164],\n",
       "        [ 27.3184, -13.2922,   0.6940,   3.9669,   1.6408,   1.6315,  -0.1891]],\n",
       "       device='cuda:0'))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0].bboxes_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_preds(preds,inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorRT Provider (ONNX Runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-22 15:07:18.276197313 [W:onnxruntime:Default, tensorrt_execution_provider.h:75 log] [2024-02-22 15:07:18 WARNING] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "2024-02-22 15:07:18.276223960 [W:onnxruntime:Default, tensorrt_execution_provider.h:75 log] [2024-02-22 15:07:18 WARNING] onnx2trt_utils.cpp:400: One or more weights outside the range of INT32 was clamped\n",
      "2024-02-22 15:07:18.357865854 [W:onnxruntime:Default, tensorrt_execution_provider.h:75 log] [2024-02-22 15:07:18 WARNING] onnx2trt_utils.cpp:374: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "2024-02-22 15:07:18.357891501 [W:onnxruntime:Default, tensorrt_execution_provider.h:75 log] [2024-02-22 15:07:18 WARNING] onnx2trt_utils.cpp:400: One or more weights outside the range of INT32 was clamped\n"
     ]
    }
   ],
   "source": [
    "ort_sess = ort.InferenceSession(model_path,providers=['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size!\n",
      "Inf: 23.366451263427734 ms\n",
      "Post: 4.77290153503418 ms\n",
      "Total: 28.179168701171875 ms\n"
     ]
    }
   ],
   "source": [
    "inputs = prepare_input({\"points\": pcl_path}, batch_size=1)[\"inputs\"]\n",
    "onnx_inputs = {'voxels': inputs[\"voxels\"][\"voxels\"].cpu().numpy(), 'coors':inputs[\"voxels\"][\"coors\"].cpu().numpy(),'num_points':inputs[\"voxels\"][\"num_points\"].cpu().numpy()}\n",
    "\n",
    "s1 = time.time()\n",
    "outputs = ort_sess.run(None, onnx_inputs)\n",
    "s2 = time.time()\n",
    "preds  = model.bbox_head.predict_by_feat(*outputs)\n",
    "print(\"Inf: {} ms\".format((s2-s1)*1000))\n",
    "print(\"Post: {} ms\".format((time.time()-s2)*1000))\n",
    "print(\"Total: {} ms\".format((time.time()-s1)*1000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Provider  (ONNX Runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_sess = ort.InferenceSession(model_path,providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size!\n",
      "Inf: 36.96608543395996 ms\n",
      "Post: 6.192684173583984 ms\n",
      "Total: 43.22385787963867 ms\n"
     ]
    }
   ],
   "source": [
    "inputs = prepare_input({\"points\": pcl_path}, batch_size=1)[\"inputs\"]\n",
    "onnx_inputs = {'voxels': inputs[\"voxels\"][\"voxels\"].cpu().numpy(), 'coors':inputs[\"voxels\"][\"coors\"].cpu().numpy(),'num_points':inputs[\"voxels\"][\"num_points\"].cpu().numpy()}\n",
    "\n",
    "s1 = time.time()\n",
    "outputs = ort_sess.run(None, onnx_inputs)\n",
    "s2 = time.time()\n",
    "preds  = model.bbox_head.predict_by_feat(*outputs)\n",
    "print(\"Inf: {} ms\".format((s2-s1)*1000))\n",
    "print(\"Post: {} ms\".format((time.time()-s2)*1000))\n",
    "print(\"Total: {} ms\".format((time.time()-s1)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU Provider  (ONNX Runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_sess = ort.InferenceSession(model_path,providers=[ 'CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size!\n",
      "Inf: 194.2272186279297 ms\n",
      "Post: 48.16293716430664 ms\n",
      "Total: 242.44952201843262 ms\n"
     ]
    }
   ],
   "source": [
    "inputs = prepare_input({\"points\": pcl_path}, batch_size=1)[\"inputs\"]\n",
    "onnx_inputs = {'voxels': inputs[\"voxels\"][\"voxels\"].cpu().numpy(), 'coors':inputs[\"voxels\"][\"coors\"].cpu().numpy(),'num_points':inputs[\"voxels\"][\"num_points\"].cpu().numpy()}\n",
    "\n",
    "s1 = time.time()\n",
    "outputs = ort_sess.run(None, onnx_inputs)\n",
    "s2 = time.time()\n",
    "preds  = model.bbox_head.predict_by_feat(*outputs)\n",
    "print(\"Inf: {} ms\".format((s2-s1)*1000))\n",
    "print(\"Post: {} ms\".format((time.time()-s2)*1000))\n",
    "print(\"Total: {} ms\".format((time.time()-s1)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TensorRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdeploy.apis.inference import inference_model\n",
    "from mmdeploy.apis.inference import get_model\n",
    "\n",
    "model_cfg = \"../configs/pointpillars/pointpillars_hv_secfpn_8xb6-160e_donaset-3d-car.py\"\n",
    "deploy_cfg = \"/root/workspace/mmdeploy/configs/mmdet3d/voxel-detection/voxel-detection_tensorrt_dynamic-kitti-32x4.py\"\n",
    "backend_files = \"end2end.engine\"\n",
    "img = \"../data/DonaSet/training/velodyne/000000.bin\"\n",
    "device = \"cuda\"\n",
    "\n",
    "tensorrt_model = get_model(model_cfg,deploy_cfg,[backend_files],img,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inf: 4.0476155281066895 ms\n",
      "Inf: 1.1346888542175293 ms\n",
      "Post: 23.787331581115723 ms\n",
      "Total: 28.96963596343994 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "times = 100\n",
    "sum_pre = 0\n",
    "sum_inf = 0\n",
    "sum_post = 0\n",
    "for i in range(times):\n",
    "    s0 = time.time()\n",
    "    inputs = prepare_input({\"points\": pcl_path}, batch_size=1,data_samples=False)[\"inputs\"]\n",
    "    s1=time.time()\n",
    "    res = tensorrt_model(inputs, mode=\"tensor\")\n",
    "    s2 =time.time()\n",
    "    preds  = model.bbox_head.predict_by_feat(res[\"cls_score\"],res[\"bbox_pred\"],res[\"dir_cls_pred\"])\n",
    "    sum_post += time.time() -s2\n",
    "    sum_inf += s2-s1\n",
    "    sum_pre += s1 - s0\n",
    "print(\"Inf: {} ms\".format((sum_pre/times)*1000))\n",
    "print(\"Inf: {} ms\".format((sum_inf/times)*1000))\n",
    "print(\"Post: {} ms\".format((sum_post/times)*1000))\n",
    "print(\"Total: {} ms\".format(((sum_inf+sum_post+sum_pre)/times)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_preds(preds,inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdeploy.apis.inference import inference_model\n",
    "from mmdeploy.apis.inference import get_model\n",
    "\n",
    "model_cfg = \"../configs/pointpillars/pointpillars_hv_secfpn_8xb6-160e_donaset-3d-car.py\"\n",
    "deploy_cfg = \"/root/workspace/mmdeploy/configs/mmdet3d/voxel-detection/voxel-detection_tensorrt_dynamic-kitti-32x4.py\"\n",
    "backend_files = \"end2end_fp16.engine\"\n",
    "img = \"../data/DonaSet/training/velodyne/000000.bin\"\n",
    "device = \"cuda\"\n",
    "\n",
    "tensorrt_model = get_model(model_cfg,deploy_cfg,[backend_files],img,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inf: 4.298429489135742 ms\n",
      "Inf: 1.263444423675537 ms\n",
      "Post: 14.704809188842773 ms\n",
      "Total: 20.266683101654053 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "times = 100\n",
    "sum_pre = 0\n",
    "sum_inf = 0\n",
    "sum_post = 0\n",
    "for i in range(times):\n",
    "    s0 = time.time()\n",
    "    inputs = prepare_input({\"points\": pcl_path}, batch_size=1,data_samples=False)[\"inputs\"]\n",
    "    s1=time.time()\n",
    "    res = tensorrt_model(inputs, mode=\"tensor\")\n",
    "    s2 =time.time()\n",
    "    preds  = model.bbox_head.predict_by_feat(res[\"cls_score\"],res[\"bbox_pred\"],res[\"dir_cls_pred\"])\n",
    "    sum_post += time.time() -s2\n",
    "    sum_inf += s2-s1\n",
    "    sum_pre += s1 - s0\n",
    "print(\"Inf: {} ms\".format((sum_pre/times)*1000))\n",
    "print(\"Inf: {} ms\".format((sum_inf/times)*1000))\n",
    "print(\"Post: {} ms\".format((sum_post/times)*1000))\n",
    "print(\"Total: {} ms\".format(((sum_inf+sum_post+sum_pre)/times)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_preds(preds,inputs)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a0c343fece975dd89087e8c2194dd4d3db28d7000f1b32ed9ed9d584dd54dbbe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
